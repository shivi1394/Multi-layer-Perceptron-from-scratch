{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cloudpickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "import scipy.sparse\n",
    "import copy\n",
    "import random\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000):\n",
    "    '''\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the neural net classifier.\n",
    "    '''\n",
    "    # Load the raw CIFAR-10 data\n",
    "    X_train, y_train, X_test, y_test = load(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    X_train = X_train.astype(np.float64)\n",
    "    X_val = X_val.astype(np.float64)\n",
    "    X_test = X_test.astype(np.float64)\n",
    "\n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2)\n",
    "    X_val = X_val.transpose(0, 3, 1, 2)\n",
    "    X_test = X_test.transpose(0, 3, 1, 2)\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train)\n",
    "\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    X_train /= std\n",
    "    X_val /= std\n",
    "    X_test /= std\n",
    "    '''\n",
    "    # covert N x 3 x 32 x 32 to N x 3072\n",
    "    X_train = np.reshape(X_train, (len(X_train), 3 * 32 * 32))\n",
    "    X_val = np.reshape(X_val, (len(X_val), 3 * 32 * 32))\n",
    "    X_test = np.reshape(X_test, (len(X_test), 3 * 32 * 32))\n",
    "    '''\n",
    "    return {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val,\n",
    "        'X_test': X_test, 'y_test': y_test,\n",
    "        'mean': mean_image, 'std': std\n",
    "    }\n",
    "\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    ''' load single batch of cifar '''\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding ='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "def get_CIFAR10(ROOT):\n",
    "    ''' load all of cifar '''\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Xte, Ytr, Yte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(file_name):\n",
    "        train_x, test_x, train_y, test_y = get_CIFAR10(\"\")\n",
    "        # covert N x 3 x 32 x 32 to N x 3072\n",
    "        train_x = np.reshape(train_x, (len(train_x), 3 * 32 * 32))\n",
    "        test_x = np.reshape(test_x, (len(test_x), 3 * 32 * 32))\n",
    "        \n",
    "        return train_x, test_x, train_y, test_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#COST FUNCTION\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z), axis=1)).T\n",
    "    return sm\n",
    "\n",
    "\n",
    "def softmax_prime(z):\n",
    "    return\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "\n",
    "def relu_prime(z):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z < 0] = 0\n",
    "    return dz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WEIGHTS INITIALIZATION FUNCTION\n",
    "def relu_weight(m, n):\n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(m, n) * np.sqrt(2) / np.sqrt(m)\n",
    "\n",
    "\n",
    "def xavier(m, n):\n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(m, n) / np.sqrt(m)\n",
    "\n",
    "\n",
    "def he(m, n):\n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(m, n) * np.sqrt(2) / np.sqrt(m + n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layers(object):\n",
    "\n",
    "    def __init__(self, n_in, n_out=10, activation_function=\"relu\"):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.set_activation_functions(act_function_name=activation_function)\n",
    "\n",
    "    def set_activation_functions(self, act_function_name=\"relu\"):\n",
    "        if act_function_name == \"relu\":\n",
    "            self.activation_function = relu\n",
    "            self.function = relu_prime\n",
    "            self.set_weight_function(weight_name=\"he\")\n",
    "        elif act_function_name == \"sigmoid\":\n",
    "            self.activation_function = sigmoid\n",
    "            self.function = sigmoid_prime\n",
    "            self.set_weight_function(weight_name=\"xavier\")\n",
    "        elif act_function_name == \"softmax\":\n",
    "            self.activation_function = softmax\n",
    "            self.function = softmax_prime\n",
    "            self.set_weight_function(weight_name=\"he\")\n",
    "\n",
    "    def set_weight_function(self, weight_name):\n",
    "        if weight_name == \"relu\":\n",
    "            self.weight_function = relu_weight\n",
    "        elif weight_name == \"xavier\":\n",
    "            self.weight_function = xavier\n",
    "        elif weight_name == \"he\":\n",
    "            self.weight_function = he\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self, n_in=784, n_out=10, l_rate=1.0):\n",
    "\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.initial_lrate = l_rate\n",
    "        self.l_rate = l_rate\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.previous_weights = []\n",
    "        self.previous_biases = []\n",
    "        self.layers = []\n",
    "        self.losses = []\n",
    "        \n",
    "\n",
    "    def layer(self, activation_function=\"relu\", n_neurons=4):\n",
    "        if len(self.layers) <= 0:\n",
    "            n_previous_neurons = self.n_in\n",
    "        else:\n",
    "            n_previous_neurons = self.layers[-1].n_out\n",
    "\n",
    "        L = Layers(n_in=n_previous_neurons, n_out=n_neurons, activation_function=activation_function)\n",
    "        self.layers.append(L)\n",
    "        \n",
    "    \n",
    "    def backpropogation_weights(self):\n",
    "        self.previous_weights.append(self.weights)\n",
    "        self.previous_biases.append(self.biases)\n",
    "    \n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        a = [X]\n",
    "        for l in range(len(self.layers)):\n",
    "            z = a[l].dot(self.weights[l]) + self.biases[l]\n",
    "            activation = self.layers[l].activation_function(z)\n",
    "            a.append(activation)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def backpropagation(self, x,y_mat,a):\n",
    "        m = x.shape[0]\n",
    "        output = a[-1]\n",
    "        \n",
    "        loss = (-1 / m) * np.sum(y_mat * np.log(output))\n",
    "        \n",
    "        deltas = []\n",
    "        delta = y_mat - output\n",
    "        deltas.append(delta)\n",
    "        \n",
    "        for l in range(len(self.layers)-1):\n",
    "            prime = self.layers[-2 - l].function(a[-2 - l])\n",
    "            w = self.weights[-1-l]\n",
    "            delta = np.dot(delta, w.T) * prime\n",
    "            deltas.append(delta)\n",
    "        \n",
    "        prev_weights = self.previous_weights.pop(0)\n",
    "        prev_biases = self.previous_biases.pop(0)\n",
    "\n",
    "        \n",
    "        for l in range(len(self.layers)-1):\n",
    "            dw = (2/m) * np.dot(a[l].T,deltas[-1-l])\n",
    "            self.weights[l] += self.l_rate * dw\n",
    "            \n",
    "            db = (1/m) * np.sum(deltas[-1-l], axis=0, keepdims=True)\n",
    "            self.biases[l] += self.l_rate * db\n",
    "   \n",
    "        self.backpropogation_weights()\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            n_cur_layer_neurons = self.layers[i].n_out\n",
    "            n_prev_layer_neurons = self.layers[i].n_in\n",
    "\n",
    "            weights = self.layers[i].weight_function(n_prev_layer_neurons, n_cur_layer_neurons)\n",
    "            self.weights.append(weights)\n",
    "\n",
    "            biases = np.zeros((1, n_cur_layer_neurons))\n",
    "            self.biases.append(biases)\n",
    "\n",
    "    def train(self, x, y, n_epoch=50000):\n",
    "        \n",
    "        self.initialize_weights()\n",
    "        self.backpropogation_weights()\n",
    "        \n",
    "        y_mat = self.oneHotIt(y)\n",
    "        \n",
    "        for i in range(n_epoch):\n",
    "            a = self.forward_propagation(x)\n",
    "            loss = self.backpropagation(x,y_mat,a)\n",
    "                       \n",
    "            if i%1000==0:\n",
    "                print('Iteration: {0}  --  Loss: {1}'.format(i,loss))\n",
    "                self.losses.append([i,loss])\n",
    "            \n",
    "    #Encode Target Label IDs to one hot vector of size m where m is the number of unique labels\n",
    "    def oneHotIt(self, Y):\n",
    "        m = Y.shape[0]\n",
    "        label = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "        label = np.array(label.todense()).T\n",
    "        return label\n",
    "    \n",
    "    def Predicted_value(self, x):\n",
    "        probs = self.forward_propagation(x)[-1]\n",
    "        preds = np.argmax(probs,axis=1)      \n",
    "        return probs,preds\n",
    "\n",
    "    def Accuracy(self, x,y):\n",
    "        prob,predicted_val = self.Predicted_value(x)\n",
    "        accuracy = sum(predicted_val == y)/(float(len(y)))\n",
    "        percentage = accuracy*100\n",
    "        return percentage\n",
    "    \n",
    "    def loss_graph(self):\n",
    "        errors = np.array(self.losses)\n",
    "        plt.plot(errors[:, 0], errors[:, 1], 'r--')\n",
    "        plt.title(\"(CIFAR-10) Loss vs Epoch (Learning rate = 1.0)\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0  --  Loss: 2.337716768928859\n",
      "Iteration: 1000  --  Loss: 2.3001272682558196\n",
      "Iteration: 2000  --  Loss: 2.300048924060527\n",
      "Iteration: 3000  --  Loss: 2.3000439573777216\n",
      "Iteration: 4000  --  Loss: 2.3000418468709296\n",
      "Iteration: 5000  --  Loss: 2.300040447189312\n",
      "Iteration: 6000  --  Loss: 2.300039442458885\n",
      "Iteration: 7000  --  Loss: 2.3000387190120386\n",
      "Iteration: 8000  --  Loss: 2.3000382079572623\n",
      "Iteration: 9000  --  Loss: 2.300037845812053\n",
      "Iteration: 10000  --  Loss: 2.3000375742415264\n",
      "Iteration: 11000  --  Loss: 2.3000373501718414\n",
      "Iteration: 12000  --  Loss: 2.300037147359945\n",
      "Iteration: 13000  --  Loss: 2.3000369514236203\n",
      "Iteration: 14000  --  Loss: 2.300036754566792\n",
      "Iteration: 15000  --  Loss: 2.3000365521235926\n",
      "Iteration: 16000  --  Loss: 2.3000363406439512\n",
      "Iteration: 17000  --  Loss: 2.3000361168497365\n",
      "Iteration: 18000  --  Loss: 2.300035876975245\n",
      "Iteration: 19000  --  Loss: 2.3000356161868867\n",
      "Iteration: 20000  --  Loss: 2.3000353278456784\n",
      "Iteration: 21000  --  Loss: 2.3000350023158815\n",
      "Iteration: 22000  --  Loss: 2.3000346247535335\n",
      "Iteration: 23000  --  Loss: 2.3000341705080483\n",
      "Iteration: 24000  --  Loss: 2.3000335942037964\n",
      "Iteration: 25000  --  Loss: 2.3000327985249833\n",
      "Iteration: 26000  --  Loss: 2.3000315140108416\n",
      "Iteration: 27000  --  Loss: 2.3000284410024086\n",
      "Iteration: 28000  --  Loss: 2.298451855116569\n",
      "Iteration: 29000  --  Loss: 2.2607475917593\n",
      "Iteration: 30000  --  Loss: 2.218081896070825\n",
      "Iteration: 31000  --  Loss: 2.183748013994039\n",
      "Iteration: 32000  --  Loss: 2.1604204993694527\n",
      "Iteration: 33000  --  Loss: 2.1371989792859853\n",
      "Iteration: 34000  --  Loss: 2.127325250774772\n",
      "Iteration: 35000  --  Loss: 2.1265403054900474\n",
      "Iteration: 36000  --  Loss: 2.10296613518176\n",
      "Iteration: 37000  --  Loss: 2.092571047508414\n",
      "Iteration: 38000  --  Loss: 2.1087646212519813\n",
      "Iteration: 39000  --  Loss: 2.0991003302911606\n",
      "Iteration: 40000  --  Loss: 2.0781903459673523\n",
      "Iteration: 41000  --  Loss: 2.0856945760817864\n",
      "Iteration: 42000  --  Loss: 2.0700141684218094\n",
      "Iteration: 43000  --  Loss: 2.065646376521925\n",
      "Iteration: 44000  --  Loss: 2.0413389286447474\n",
      "Iteration: 45000  --  Loss: 2.037655465609746\n",
      "Iteration: 46000  --  Loss: 2.0211631529261265\n",
      "Iteration: 47000  --  Loss: 2.0394212927149904\n",
      "Iteration: 48000  --  Loss: 2.0044636924945674\n",
      "Iteration: 49000  --  Loss: 2.004033063192254\n",
      "Iteration: 50000  --  Loss: 2.000999770675411\n",
      "Iteration: 51000  --  Loss: 1.9917421817432883\n",
      "Iteration: 52000  --  Loss: 1.991680936490878\n",
      "Iteration: 53000  --  Loss: 1.980503112651195\n",
      "Iteration: 54000  --  Loss: 1.980667414212518\n",
      "Iteration: 55000  --  Loss: 1.9734173862229683\n",
      "Iteration: 56000  --  Loss: 1.977009083627647\n",
      "Iteration: 57000  --  Loss: 1.967185641619293\n",
      "Iteration: 58000  --  Loss: 2.038252748347511\n",
      "Iteration: 59000  --  Loss: 1.997129260047195\n",
      "Iteration: 60000  --  Loss: 1.9639273938198325\n",
      "Iteration: 61000  --  Loss: 1.9615479608731385\n",
      "Iteration: 62000  --  Loss: 1.961781968872479\n",
      "Iteration: 63000  --  Loss: 1.9595991207982448\n",
      "Iteration: 64000  --  Loss: 1.9576299473124104\n",
      "Iteration: 65000  --  Loss: 1.958103074150477\n",
      "Iteration: 66000  --  Loss: 1.957470981581995\n",
      "Iteration: 67000  --  Loss: 1.9592945299961795\n",
      "Iteration: 68000  --  Loss: 1.9566325265796436\n",
      "Iteration: 69000  --  Loss: 1.9536761852495865\n",
      "Iteration: 70000  --  Loss: 1.9561699768892196\n",
      "Iteration: 71000  --  Loss: 1.9522849944859086\n",
      "Iteration: 72000  --  Loss: 1.952770476596797\n",
      "Iteration: 73000  --  Loss: 1.9628915067599342\n",
      "Iteration: 74000  --  Loss: 1.9538350243583584\n",
      "Iteration: 75000  --  Loss: 1.9519854598525708\n",
      "Iteration: 76000  --  Loss: 1.9587077892658435\n",
      "Iteration: 77000  --  Loss: 1.94648979540857\n",
      "Iteration: 78000  --  Loss: 1.9514779211797222\n",
      "Iteration: 79000  --  Loss: 1.9577829921641483\n",
      "Iteration: 80000  --  Loss: 1.9749416228830783\n",
      "Iteration: 81000  --  Loss: 1.9500658145780085\n",
      "Iteration: 82000  --  Loss: 1.9507701612364494\n",
      "Iteration: 83000  --  Loss: 1.9512030757534218\n",
      "Iteration: 84000  --  Loss: 1.9517116945099302\n",
      "Iteration: 85000  --  Loss: 1.9442483899576821\n",
      "Iteration: 86000  --  Loss: 1.9379224037869989\n",
      "Iteration: 87000  --  Loss: 1.9392048191566835\n",
      "Iteration: 88000  --  Loss: 1.9398228607253274\n",
      "Iteration: 89000  --  Loss: 1.9386224360360558\n",
      "Iteration: 90000  --  Loss: 1.9384933039334828\n",
      "Iteration: 91000  --  Loss: 1.9481122607358972\n",
      "Iteration: 92000  --  Loss: 1.934212856774202\n",
      "Iteration: 93000  --  Loss: 1.9403831185053582\n",
      "Iteration: 94000  --  Loss: 1.934796498752011\n",
      "Iteration: 95000  --  Loss: 1.9289003363960715\n",
      "Iteration: 96000  --  Loss: 1.9311189769686024\n",
      "Iteration: 97000  --  Loss: 1.9270226834811834\n",
      "Iteration: 98000  --  Loss: 1.930229808213686\n",
      "Iteration: 99000  --  Loss: 1.9321561420318523\n",
      "Iteration: 100000  --  Loss: 1.9295722898767864\n",
      "Iteration: 101000  --  Loss: 1.929790831442196\n",
      "Iteration: 102000  --  Loss: 1.9257536118148295\n",
      "Iteration: 103000  --  Loss: 1.9279323665404537\n",
      "Iteration: 104000  --  Loss: 1.924898782322025\n",
      "Iteration: 105000  --  Loss: 1.928222516015273\n",
      "Iteration: 106000  --  Loss: 1.9255425886775417\n",
      "Iteration: 107000  --  Loss: 1.9277433958041834\n",
      "Iteration: 108000  --  Loss: 1.935325148634745\n",
      "Iteration: 109000  --  Loss: 1.9256332453398677\n",
      "Iteration: 110000  --  Loss: 1.9286868396209433\n",
      "Iteration: 111000  --  Loss: 1.924186455700157\n",
      "Iteration: 112000  --  Loss: 1.9250786252900258\n",
      "Iteration: 113000  --  Loss: 1.9276890035770273\n",
      "Iteration: 114000  --  Loss: 1.9840768865770506\n",
      "Iteration: 115000  --  Loss: 1.926060166778322\n",
      "Iteration: 116000  --  Loss: 1.9238137353906617\n",
      "Iteration: 117000  --  Loss: 1.9262277940315535\n",
      "Iteration: 118000  --  Loss: 1.92399058391688\n",
      "Iteration: 119000  --  Loss: 1.9232949093744784\n",
      "Iteration: 120000  --  Loss: 1.9243868174595828\n",
      "Iteration: 121000  --  Loss: 1.9233901668761264\n",
      "Iteration: 122000  --  Loss: 1.9160207009042225\n",
      "Iteration: 123000  --  Loss: 1.919458206886353\n",
      "Iteration: 124000  --  Loss: 1.921494262702864\n",
      "Iteration: 125000  --  Loss: 1.9157936505458375\n",
      "Iteration: 126000  --  Loss: 1.9183750409752147\n",
      "Iteration: 127000  --  Loss: 1.917915643006568\n",
      "Iteration: 128000  --  Loss: 1.9118037860450796\n",
      "Iteration: 129000  --  Loss: 1.9097071980890128\n",
      "Iteration: 130000  --  Loss: 1.9083593539715837\n",
      "Iteration: 131000  --  Loss: 1.9073307927739742\n",
      "Iteration: 132000  --  Loss: 1.910796923845371\n",
      "Iteration: 133000  --  Loss: 1.9094183868843113\n",
      "Iteration: 134000  --  Loss: 1.9054817092092604\n",
      "Iteration: 135000  --  Loss: 1.9022011446693528\n",
      "Iteration: 136000  --  Loss: 1.9056731908462867\n",
      "Iteration: 137000  --  Loss: 1.8988111693901486\n",
      "Iteration: 138000  --  Loss: 1.899737100585723\n",
      "Iteration: 139000  --  Loss: 1.897607617867328\n",
      "Iteration: 140000  --  Loss: 1.8955126818906125\n",
      "Iteration: 141000  --  Loss: 1.896297234483792\n",
      "Iteration: 142000  --  Loss: 1.9089269761830825\n",
      "Iteration: 143000  --  Loss: 1.8924940578119276\n",
      "Iteration: 144000  --  Loss: 1.8899671107191625\n",
      "Iteration: 145000  --  Loss: 1.89098063284733\n",
      "Iteration: 146000  --  Loss: 1.8896686582252957\n",
      "Iteration: 147000  --  Loss: 1.8905886327956032\n",
      "Iteration: 148000  --  Loss: 1.889421228017264\n",
      "Iteration: 149000  --  Loss: 1.886855570517216\n",
      "Iteration: 150000  --  Loss: 1.902153757984244\n",
      "Iteration: 151000  --  Loss: 1.8881469013114687\n",
      "Iteration: 152000  --  Loss: 1.8914584012722329\n",
      "Iteration: 153000  --  Loss: 1.8840078972674792\n",
      "Iteration: 154000  --  Loss: 1.882591274048294\n",
      "Iteration: 155000  --  Loss: 1.8826264110389204\n",
      "Iteration: 156000  --  Loss: 1.88237908181569\n",
      "Iteration: 157000  --  Loss: 1.8828971489228643\n",
      "Iteration: 158000  --  Loss: 1.8815980579879352\n",
      "Iteration: 159000  --  Loss: 1.8789159286804908\n",
      "Iteration: 160000  --  Loss: 1.8789076536128053\n",
      "Iteration: 161000  --  Loss: 1.8796507257425457\n",
      "Iteration: 162000  --  Loss: 1.8787369480899438\n",
      "Iteration: 163000  --  Loss: 1.8779144574080393\n",
      "Iteration: 164000  --  Loss: 1.876619075478428\n",
      "Iteration: 165000  --  Loss: 1.8775777371627134\n",
      "Iteration: 166000  --  Loss: 1.8760785724894027\n",
      "Iteration: 167000  --  Loss: 1.8785498126064324\n",
      "Iteration: 168000  --  Loss: 1.874970213110917\n",
      "Iteration: 169000  --  Loss: 1.877136855306892\n",
      "Iteration: 170000  --  Loss: 1.8739729554861122\n",
      "Iteration: 171000  --  Loss: 1.874342514198979\n",
      "Iteration: 172000  --  Loss: 1.8731705819983062\n",
      "Iteration: 173000  --  Loss: 1.872500732930825\n",
      "Iteration: 174000  --  Loss: 1.8731074955149432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 175000  --  Loss: 1.8816153218194493\n",
      "Iteration: 176000  --  Loss: 1.8720101187334974\n",
      "Iteration: 177000  --  Loss: 1.872560987335447\n",
      "Iteration: 178000  --  Loss: 1.877934002703146\n",
      "Iteration: 179000  --  Loss: 1.8730036549602864\n",
      "Iteration: 180000  --  Loss: 1.8711940458626113\n",
      "Iteration: 181000  --  Loss: 1.8714094859208998\n",
      "Iteration: 182000  --  Loss: 1.869240003306478\n",
      "Iteration: 183000  --  Loss: 1.86985377997679\n",
      "Iteration: 184000  --  Loss: 1.8753350244042077\n",
      "Iteration: 185000  --  Loss: 1.8678326060913815\n",
      "Iteration: 186000  --  Loss: 1.8686803676571784\n",
      "Iteration: 187000  --  Loss: 1.8680599461511007\n",
      "Iteration: 188000  --  Loss: 1.8670966734803829\n",
      "Iteration: 189000  --  Loss: 1.8783618986613586\n",
      "Iteration: 190000  --  Loss: 1.8660395254301816\n",
      "Iteration: 191000  --  Loss: 1.8656915181894516\n",
      "Iteration: 192000  --  Loss: 1.87084532078327\n",
      "Iteration: 193000  --  Loss: 1.8977597969175017\n",
      "Iteration: 194000  --  Loss: 1.8654767417001978\n",
      "Iteration: 195000  --  Loss: 1.8660521771473981\n",
      "Iteration: 196000  --  Loss: 1.8658217171160822\n",
      "Iteration: 197000  --  Loss: 1.8641371694072244\n",
      "Iteration: 198000  --  Loss: 1.8651402349082793\n",
      "Iteration: 199000  --  Loss: 1.8648578958728796\n",
      "Iteration: 200000  --  Loss: 1.8785947997151233\n",
      "Iteration: 201000  --  Loss: 1.8675459550963858\n",
      "Iteration: 202000  --  Loss: 1.863658178092156\n",
      "Iteration: 203000  --  Loss: 1.863600015791827\n",
      "Iteration: 204000  --  Loss: 1.8651678126943467\n",
      "Iteration: 205000  --  Loss: 1.863887630410394\n",
      "Iteration: 206000  --  Loss: 1.863078291485031\n",
      "Iteration: 207000  --  Loss: 1.8648214781597823\n",
      "Iteration: 208000  --  Loss: 1.8645465916895778\n",
      "Iteration: 209000  --  Loss: 1.864564063165727\n",
      "Iteration: 210000  --  Loss: 1.8635170026924526\n",
      "Iteration: 211000  --  Loss: 1.8704323512861152\n",
      "Iteration: 212000  --  Loss: 1.8622108617730455\n",
      "Iteration: 213000  --  Loss: 1.8624626429442341\n",
      "Iteration: 214000  --  Loss: 1.867413561306417\n",
      "Iteration: 215000  --  Loss: 1.862994151657252\n",
      "Iteration: 216000  --  Loss: 1.8604964919419196\n",
      "Iteration: 217000  --  Loss: 1.8597928628707467\n",
      "Iteration: 218000  --  Loss: 1.8610189935569523\n",
      "Iteration: 219000  --  Loss: 1.8602022281762174\n",
      "Iteration: 220000  --  Loss: 1.859984939726935\n",
      "Iteration: 221000  --  Loss: 1.8587426206076565\n",
      "Iteration: 222000  --  Loss: 1.8599926116254837\n",
      "Iteration: 223000  --  Loss: 1.8583246134429874\n",
      "Iteration: 224000  --  Loss: 1.863834809717428\n",
      "Iteration: 225000  --  Loss: 1.8630482096351488\n",
      "Iteration: 226000  --  Loss: 1.8586421981536\n",
      "Iteration: 227000  --  Loss: 1.8579044158221691\n",
      "Iteration: 228000  --  Loss: 1.857760344450804\n",
      "Iteration: 229000  --  Loss: 1.8557099012375575\n",
      "Iteration: 230000  --  Loss: 1.8554180147250876\n",
      "Iteration: 231000  --  Loss: 1.8566262360420587\n",
      "Iteration: 232000  --  Loss: 1.8558067955127715\n",
      "Iteration: 233000  --  Loss: 1.8592297948450995\n",
      "Iteration: 234000  --  Loss: 1.854373165913819\n",
      "Iteration: 235000  --  Loss: 1.8545460368692805\n",
      "Iteration: 236000  --  Loss: 1.8546207729550448\n",
      "Iteration: 237000  --  Loss: 1.8558404235075714\n",
      "Iteration: 238000  --  Loss: 1.8550377542861833\n",
      "Iteration: 239000  --  Loss: 1.8648143391953955\n",
      "Iteration: 240000  --  Loss: 1.8543334811694685\n",
      "Iteration: 241000  --  Loss: 1.8538740519493901\n",
      "Iteration: 242000  --  Loss: 1.8665274280251054\n",
      "Iteration: 243000  --  Loss: 1.8549968732749804\n",
      "Iteration: 244000  --  Loss: 1.854588451546931\n",
      "Iteration: 245000  --  Loss: 1.8648904800298207\n",
      "Iteration: 246000  --  Loss: 1.854210109113764\n",
      "Iteration: 247000  --  Loss: 1.8557044843993913\n",
      "Iteration: 248000  --  Loss: 1.8533512479567873\n",
      "Iteration: 249000  --  Loss: 1.8538398020035871\n",
      "Iteration: 250000  --  Loss: 1.853973791272331\n",
      "Iteration: 251000  --  Loss: 1.852533054652437\n",
      "Iteration: 252000  --  Loss: 1.8530082983444578\n",
      "Iteration: 253000  --  Loss: 1.8707276147786596\n",
      "Iteration: 254000  --  Loss: 1.8532331976803496\n",
      "Iteration: 255000  --  Loss: 1.8523575672931931\n",
      "Iteration: 256000  --  Loss: 1.8519329872462336\n",
      "Iteration: 257000  --  Loss: 1.8949432115572955\n",
      "Iteration: 258000  --  Loss: 1.8515817243946462\n",
      "Iteration: 259000  --  Loss: 1.8537077656926013\n",
      "Iteration: 260000  --  Loss: 1.8526239516677785\n",
      "Iteration: 261000  --  Loss: 1.852325615091907\n",
      "Iteration: 262000  --  Loss: 1.8722224207711098\n",
      "Iteration: 263000  --  Loss: 1.853642641507213\n",
      "Iteration: 264000  --  Loss: 1.8523821959815852\n",
      "Iteration: 265000  --  Loss: 1.8506727219744403\n",
      "Iteration: 266000  --  Loss: 1.850597869571222\n",
      "Iteration: 267000  --  Loss: 1.8500128179218567\n",
      "Iteration: 268000  --  Loss: 1.8500908896540678\n",
      "Iteration: 269000  --  Loss: 1.8499856655343514\n",
      "Iteration: 270000  --  Loss: 1.8512349064241163\n",
      "Iteration: 271000  --  Loss: 1.851920807630948\n",
      "Iteration: 272000  --  Loss: 1.8501726054388152\n",
      "Iteration: 273000  --  Loss: 1.851195500055323\n",
      "Iteration: 274000  --  Loss: 1.8499411906789782\n",
      "Iteration: 275000  --  Loss: 1.8505682277658713\n",
      "Iteration: 276000  --  Loss: 1.8504096536705836\n",
      "Iteration: 277000  --  Loss: 1.8505884317089154\n",
      "Iteration: 278000  --  Loss: 1.851475199117901\n",
      "Iteration: 279000  --  Loss: 1.8505714795389403\n",
      "Iteration: 280000  --  Loss: 1.8487834590446928\n",
      "Iteration: 281000  --  Loss: 1.8495509826310108\n",
      "Iteration: 282000  --  Loss: 1.8506964222869255\n",
      "Iteration: 283000  --  Loss: 1.848436500858283\n",
      "Iteration: 284000  --  Loss: 1.850306945536058\n",
      "Iteration: 285000  --  Loss: 1.848657786355263\n",
      "Iteration: 286000  --  Loss: 1.8489745575790444\n",
      "Iteration: 287000  --  Loss: 1.8819742115764206\n",
      "Iteration: 288000  --  Loss: 1.8488670198480017\n",
      "Iteration: 289000  --  Loss: 1.8477170537866796\n",
      "Iteration: 290000  --  Loss: 1.8486467320008986\n",
      "Iteration: 291000  --  Loss: 1.8492346445487928\n",
      "Iteration: 292000  --  Loss: 1.8479858467516743\n",
      "Iteration: 293000  --  Loss: 1.8477756001078722\n",
      "Iteration: 294000  --  Loss: 1.849095992264118\n",
      "Iteration: 295000  --  Loss: 1.8488187890762238\n",
      "Iteration: 296000  --  Loss: 1.8475602917131957\n",
      "Iteration: 297000  --  Loss: 1.8486012214310976\n",
      "Iteration: 298000  --  Loss: 1.8486199234184937\n",
      "Iteration: 299000  --  Loss: 1.8476446045993076\n",
      "Iteration: 300000  --  Loss: 1.848302234970033\n",
      "Iteration: 301000  --  Loss: 1.8474753443282927\n",
      "Iteration: 302000  --  Loss: 1.8479326049416207\n",
      "Iteration: 303000  --  Loss: 1.8476503881357385\n",
      "Iteration: 304000  --  Loss: 1.849522228179038\n",
      "Iteration: 305000  --  Loss: 1.8476003333380566\n",
      "Iteration: 306000  --  Loss: 1.8468625777966317\n",
      "Iteration: 307000  --  Loss: 1.8464345051161442\n",
      "Iteration: 308000  --  Loss: 1.866773401305488\n",
      "Iteration: 309000  --  Loss: 1.847972858810923\n",
      "Iteration: 310000  --  Loss: 1.8463082108455453\n",
      "Iteration: 311000  --  Loss: 1.8485488990314762\n",
      "Iteration: 312000  --  Loss: 1.8464321764036367\n",
      "Iteration: 313000  --  Loss: 1.8462080386117168\n",
      "Iteration: 314000  --  Loss: 1.846523439711142\n",
      "Iteration: 315000  --  Loss: 1.8462774757304912\n",
      "Iteration: 316000  --  Loss: 1.847145515616289\n",
      "Iteration: 317000  --  Loss: 1.8474984766654712\n",
      "Iteration: 318000  --  Loss: 1.8473467867521414\n",
      "Iteration: 319000  --  Loss: 1.8460115684836418\n",
      "Iteration: 320000  --  Loss: 1.8455170968354615\n",
      "Iteration: 321000  --  Loss: 1.8471852446211223\n",
      "Iteration: 322000  --  Loss: 1.8466341820519023\n",
      "Iteration: 323000  --  Loss: 1.8462941770399146\n",
      "Iteration: 324000  --  Loss: 1.8461908316960631\n",
      "Iteration: 325000  --  Loss: 1.846327386149846\n",
      "Iteration: 326000  --  Loss: 1.8818906472819785\n",
      "Iteration: 327000  --  Loss: 1.8461002828579895\n",
      "Iteration: 328000  --  Loss: 1.846648577064875\n",
      "Iteration: 329000  --  Loss: 1.8474172221831124\n",
      "Iteration: 330000  --  Loss: 1.8462074099824375\n",
      "Iteration: 331000  --  Loss: 1.8457733580720974\n",
      "Iteration: 332000  --  Loss: 1.8460609606359477\n",
      "Iteration: 333000  --  Loss: 1.8456910264327169\n",
      "Iteration: 334000  --  Loss: 1.8459669467552635\n",
      "Iteration: 335000  --  Loss: 1.8449642189445243\n",
      "Iteration: 336000  --  Loss: 1.8452399551051726\n",
      "Iteration: 337000  --  Loss: 1.845954802000967\n",
      "Iteration: 338000  --  Loss: 1.8453580807977223\n",
      "Iteration: 339000  --  Loss: 1.845925075942423\n",
      "Iteration: 340000  --  Loss: 1.8472302313247129\n",
      "Iteration: 341000  --  Loss: 1.8455463725325125\n",
      "Iteration: 342000  --  Loss: 1.8469883677613153\n",
      "Iteration: 343000  --  Loss: 1.8451808482436425\n",
      "Iteration: 344000  --  Loss: 1.844683954514098\n",
      "Iteration: 345000  --  Loss: 1.8462445876198932\n",
      "Iteration: 346000  --  Loss: 1.8443792429903256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 347000  --  Loss: 1.8443869090622196\n",
      "Iteration: 348000  --  Loss: 1.8451562945026967\n",
      "Iteration: 349000  --  Loss: 1.844486426844278\n",
      "Iteration: 350000  --  Loss: 1.8464172763747704\n",
      "Iteration: 351000  --  Loss: 1.845532251837677\n",
      "Iteration: 352000  --  Loss: 1.844378772086917\n",
      "Iteration: 353000  --  Loss: 1.8447518787833794\n",
      "Iteration: 354000  --  Loss: 1.8445588949503324\n",
      "Iteration: 355000  --  Loss: 1.844652014334708\n",
      "Iteration: 356000  --  Loss: 1.8444371687581897\n",
      "Iteration: 357000  --  Loss: 1.8448967414012392\n",
      "Iteration: 358000  --  Loss: 1.844709008225108\n",
      "Iteration: 359000  --  Loss: 1.8455942124714608\n",
      "Iteration: 360000  --  Loss: 1.8448779603222003\n",
      "Iteration: 361000  --  Loss: 1.8440854797681754\n",
      "Iteration: 362000  --  Loss: 1.8448748269999689\n",
      "Iteration: 363000  --  Loss: 1.84473573216639\n",
      "Iteration: 364000  --  Loss: 1.844315123145152\n",
      "Iteration: 365000  --  Loss: 1.8449291344490548\n",
      "Iteration: 366000  --  Loss: 1.8435323405465789\n",
      "Iteration: 367000  --  Loss: 1.8442820947705219\n",
      "Iteration: 368000  --  Loss: 1.844977618096992\n",
      "Iteration: 369000  --  Loss: 1.843913151688829\n",
      "Iteration: 370000  --  Loss: 1.8442692087427328\n",
      "Iteration: 371000  --  Loss: 1.845127854106821\n",
      "Iteration: 372000  --  Loss: 1.8622012239698333\n",
      "Iteration: 373000  --  Loss: 1.8442774605416337\n",
      "Iteration: 374000  --  Loss: 1.843835310047184\n",
      "Iteration: 375000  --  Loss: 1.8451234647015267\n",
      "Iteration: 376000  --  Loss: 1.8447710993341344\n",
      "Iteration: 377000  --  Loss: 1.8444217033251302\n",
      "Iteration: 378000  --  Loss: 1.843653430004807\n",
      "Iteration: 379000  --  Loss: 1.8439683357771774\n",
      "Iteration: 380000  --  Loss: 1.8440270131721501\n",
      "Iteration: 381000  --  Loss: 1.8444395799762727\n",
      "Iteration: 382000  --  Loss: 1.844422136298503\n",
      "Iteration: 383000  --  Loss: 1.8433015258378176\n",
      "Iteration: 384000  --  Loss: 1.8446748533814505\n",
      "Iteration: 385000  --  Loss: 1.8444989936921439\n",
      "Iteration: 386000  --  Loss: 1.8445548343294649\n",
      "Iteration: 387000  --  Loss: 1.8447996194673597\n",
      "Iteration: 388000  --  Loss: 1.8438082087991157\n",
      "Iteration: 389000  --  Loss: 1.8432700862876268\n",
      "Iteration: 390000  --  Loss: 1.8454895837963277\n",
      "Iteration: 391000  --  Loss: 1.843279279469497\n",
      "Iteration: 392000  --  Loss: 1.8449946748830173\n",
      "Iteration: 393000  --  Loss: 1.8444942041554797\n",
      "Iteration: 394000  --  Loss: 1.8433953819863045\n",
      "Iteration: 395000  --  Loss: 1.8436503160775064\n",
      "Iteration: 396000  --  Loss: 1.8440224787664727\n",
      "Iteration: 397000  --  Loss: 1.843893019506669\n",
      "Iteration: 398000  --  Loss: 1.8434062015195394\n",
      "Iteration: 399000  --  Loss: 1.8447754188565562\n",
      "Iteration: 400000  --  Loss: 1.843541773221794\n",
      "Iteration: 401000  --  Loss: 1.8449676331063336\n",
      "Iteration: 402000  --  Loss: 1.843130856503775\n",
      "Iteration: 403000  --  Loss: 1.8440568224039455\n",
      "Iteration: 404000  --  Loss: 1.843766311832134\n",
      "Iteration: 405000  --  Loss: 1.8428231160691437\n",
      "Iteration: 406000  --  Loss: 1.8438068512996957\n",
      "Iteration: 407000  --  Loss: 1.8435516269772705\n",
      "Iteration: 408000  --  Loss: 1.8443762963151848\n",
      "Iteration: 409000  --  Loss: 1.8436591717415596\n",
      "Iteration: 410000  --  Loss: 1.8430378279789448\n",
      "Iteration: 411000  --  Loss: 1.845449677544661\n",
      "Iteration: 412000  --  Loss: 1.8424259978160022\n",
      "Iteration: 413000  --  Loss: 1.8438390294068223\n",
      "Iteration: 414000  --  Loss: 1.842716285633086\n",
      "Iteration: 415000  --  Loss: 1.8437635896123377\n",
      "Iteration: 416000  --  Loss: 1.8429019693931779\n",
      "Iteration: 417000  --  Loss: 1.8424049095913675\n",
      "Iteration: 418000  --  Loss: 1.8428690036593445\n",
      "Iteration: 419000  --  Loss: 1.842543941161336\n",
      "Iteration: 420000  --  Loss: 1.842715964661012\n",
      "Iteration: 421000  --  Loss: 1.8439841281445752\n",
      "Iteration: 422000  --  Loss: 1.8422703316353253\n",
      "Iteration: 423000  --  Loss: 1.8424230622462303\n",
      "Iteration: 424000  --  Loss: 1.8425606431743575\n",
      "Iteration: 425000  --  Loss: 1.845937634775457\n",
      "Iteration: 426000  --  Loss: 1.8436754854998219\n",
      "Iteration: 427000  --  Loss: 1.8436176083600582\n",
      "Iteration: 428000  --  Loss: 1.8424169606182375\n",
      "Iteration: 429000  --  Loss: 1.8432151272932746\n",
      "Iteration: 430000  --  Loss: 1.842898192370541\n",
      "Iteration: 431000  --  Loss: 1.8449812342921945\n",
      "Iteration: 432000  --  Loss: 1.8454383203800344\n",
      "Iteration: 433000  --  Loss: 1.843425284508803\n",
      "Iteration: 434000  --  Loss: 1.843234432651663\n",
      "Iteration: 435000  --  Loss: 1.843554559848238\n",
      "Iteration: 436000  --  Loss: 1.8431610754140835\n",
      "Iteration: 437000  --  Loss: 1.8429955345399966\n",
      "Iteration: 438000  --  Loss: 1.8422882883277625\n",
      "Iteration: 439000  --  Loss: 1.841840520856779\n",
      "Iteration: 440000  --  Loss: 1.8428414712063639\n",
      "Iteration: 441000  --  Loss: 1.8415809402499015\n",
      "Iteration: 442000  --  Loss: 1.8431714207215852\n",
      "Iteration: 443000  --  Loss: 1.8434745834824817\n",
      "Iteration: 444000  --  Loss: 1.8423865091788825\n",
      "Iteration: 445000  --  Loss: 1.841962823658004\n",
      "Iteration: 446000  --  Loss: 1.842797231086156\n",
      "Iteration: 447000  --  Loss: 1.8437911767368798\n",
      "Iteration: 448000  --  Loss: 1.8435875388744858\n",
      "Iteration: 449000  --  Loss: 1.8417109077007237\n",
      "Iteration: 450000  --  Loss: 1.8434180266287603\n",
      "Iteration: 451000  --  Loss: 1.842831942523725\n",
      "Iteration: 452000  --  Loss: 1.8415877133616574\n",
      "Iteration: 453000  --  Loss: 1.843801426269354\n",
      "Iteration: 454000  --  Loss: 1.8422243702474892\n",
      "Iteration: 455000  --  Loss: 1.8427048867534512\n",
      "Iteration: 456000  --  Loss: 1.8424283823433447\n",
      "Iteration: 457000  --  Loss: 1.8429104740118696\n",
      "Iteration: 458000  --  Loss: 1.842771728251805\n",
      "Iteration: 459000  --  Loss: 1.841915161956733\n",
      "Iteration: 460000  --  Loss: 1.841619803161669\n",
      "Iteration: 461000  --  Loss: 1.841710779473854\n",
      "Iteration: 462000  --  Loss: 1.8419035996973672\n",
      "Iteration: 463000  --  Loss: 1.8426524463718392\n",
      "Iteration: 464000  --  Loss: 1.841582069827145\n",
      "Iteration: 465000  --  Loss: 1.8423646910541733\n",
      "Iteration: 466000  --  Loss: 1.8430006865758342\n",
      "Iteration: 467000  --  Loss: 1.8411953499317284\n",
      "Iteration: 468000  --  Loss: 1.8435177582062143\n",
      "Iteration: 469000  --  Loss: 1.8437011988723464\n",
      "Iteration: 470000  --  Loss: 1.8441610964145687\n",
      "Iteration: 471000  --  Loss: 1.8425670453095677\n",
      "Iteration: 472000  --  Loss: 1.8431534847013806\n",
      "Iteration: 473000  --  Loss: 1.8410454778985053\n",
      "Iteration: 474000  --  Loss: 1.8420896902829995\n",
      "Iteration: 475000  --  Loss: 1.841818283362638\n",
      "Iteration: 476000  --  Loss: 1.842593530615238\n",
      "Iteration: 477000  --  Loss: 1.841272123131331\n",
      "Iteration: 478000  --  Loss: 1.8411694697692906\n",
      "Iteration: 479000  --  Loss: 1.841281024528793\n",
      "Iteration: 480000  --  Loss: 1.8415130393659087\n",
      "Iteration: 481000  --  Loss: 1.8422732555218042\n",
      "Iteration: 482000  --  Loss: 1.8416337175061455\n",
      "Iteration: 483000  --  Loss: 1.8429896597374786\n",
      "Iteration: 484000  --  Loss: 1.8422927763140513\n",
      "Iteration: 485000  --  Loss: 1.8414618626975472\n",
      "Iteration: 486000  --  Loss: 1.841588942218796\n",
      "Iteration: 487000  --  Loss: 1.842186772573935\n",
      "Iteration: 488000  --  Loss: 1.8422899041942458\n",
      "Iteration: 489000  --  Loss: 1.8412622068821194\n",
      "Iteration: 490000  --  Loss: 1.8409546717122103\n",
      "Iteration: 491000  --  Loss: 1.8401478187206108\n",
      "Iteration: 492000  --  Loss: 1.8401531187496671\n",
      "Iteration: 493000  --  Loss: 1.8416220739296643\n",
      "Iteration: 494000  --  Loss: 1.8398791229409963\n",
      "Iteration: 495000  --  Loss: 1.8398573385649062\n",
      "Iteration: 496000  --  Loss: 1.8399770269160098\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# mnist data set\n",
    "train_x, test_x, train_y, test_y = load(file_name=\"cifar\")\n",
    "    \n",
    "X_train = train_x[:1000] / 255.\n",
    "y_train = train_y[:1000].astype(int)\n",
    "X_test = test_x[:100] / 255.\n",
    "y_test = test_y[:100].astype(int)\n",
    "\n",
    "n_in = X_train.shape[1]\n",
    "\n",
    "nn = Neural_Network(n_in=n_in, n_out=10)\n",
    "\n",
    "#Randomly assigning activation functions to the hidden layers\n",
    "List = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "activation_func = random.choice(List)\n",
    "nn.layer(activation_func, n_neurons=80)\n",
    "nn.layer(activation_func, n_neurons=30)\n",
    "nn.layer(activation_function=\"softmax\", n_neurons=10)\n",
    "\n",
    "nn.train(X_train, y_train, n_epoch=500000)\n",
    "\n",
    "train_accuracy = nn.Accuracy(X_train,y_train)\n",
    "test_accuracy = nn.Accuracy(X_test,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy: {0:0.2f} %'.format(train_accuracy))\n",
    "print('Test Accuracy: {0:0.2f} %'.format(test_accuracy))\n",
    "nn.loss_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
