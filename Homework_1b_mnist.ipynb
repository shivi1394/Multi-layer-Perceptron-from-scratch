{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cloudpickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "import scipy.sparse\n",
    "import copy\n",
    "import random\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist(data_file=\"mnist.data\", test_size=0.10, random_state=0):\n",
    "    mnist = pickle.load(open(data_file, \"rb\"))\n",
    "    return train_test_split(mnist['data'], mnist['target'], test_size=test_size,\n",
    "                            random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(file_name):\n",
    "    file_name = \"mnist.data\"\n",
    "    return load_mnist(data_file=file_name, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#COST FUNCTION\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z), axis=1)).T\n",
    "    return sm\n",
    "\n",
    "\n",
    "def softmax_prime(z):\n",
    "    return\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "\n",
    "def relu_prime(z):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z < 0] = 0\n",
    "    return dz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WEIGHTS INITIALIZATION FUNCTION\n",
    "def relu_weight(m, n):\n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(m, n) * np.sqrt(2) / np.sqrt(m)\n",
    "\n",
    "\n",
    "def xavier(m, n):\n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(m, n) / np.sqrt(m)\n",
    "\n",
    "\n",
    "def he(m, n):\n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(m, n) * np.sqrt(2) / np.sqrt(m + n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layers(object):\n",
    "\n",
    "    def __init__(self, n_in, n_out=10, activation_function=\"relu\"):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.set_activation_functions(act_function_name=activation_function)\n",
    "\n",
    "    def set_activation_functions(self, act_function_name=\"relu\"):\n",
    "        if act_function_name == \"relu\":\n",
    "            self.activation_function = relu\n",
    "            self.function = relu_prime\n",
    "            self.set_weight_function(weight_name=\"he\")\n",
    "        elif act_function_name == \"sigmoid\":\n",
    "            self.activation_function = sigmoid\n",
    "            self.function = sigmoid_prime\n",
    "            self.set_weight_function(weight_name=\"xavier\")\n",
    "        elif act_function_name == \"softmax\":\n",
    "            self.activation_function = softmax\n",
    "            self.function = softmax_prime\n",
    "            self.set_weight_function(weight_name=\"he\")\n",
    "\n",
    "    def set_weight_function(self, weight_name):\n",
    "        if weight_name == \"relu\":\n",
    "            self.weight_function = relu_weight\n",
    "        elif weight_name == \"xavier\":\n",
    "            self.weight_function = xavier\n",
    "        elif weight_name == \"he\":\n",
    "            self.weight_function = he\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self, n_in=784, n_out=10, l_rate=0.1):\n",
    "\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.initial_lrate = l_rate\n",
    "        self.l_rate = l_rate\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.previous_weights = []\n",
    "        self.previous_biases = []\n",
    "        self.layers = []\n",
    "        self.losses = []\n",
    "        \n",
    "\n",
    "    def layer(self, activation_function=\"relu\", n_neurons=4):\n",
    "        if len(self.layers) <= 0:\n",
    "            n_previous_neurons = self.n_in\n",
    "        else:\n",
    "            n_previous_neurons = self.layers[-1].n_out\n",
    "\n",
    "        L = Layers(n_in=n_previous_neurons, n_out=n_neurons, activation_function=activation_function)\n",
    "        self.layers.append(L)\n",
    "        \n",
    "    \n",
    "    def backpropogation_weights(self):\n",
    "        self.previous_weights.append(self.weights)\n",
    "        self.previous_biases.append(self.biases)\n",
    "    \n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        a = [X]\n",
    "        for l in range(len(self.layers)):\n",
    "            z = a[l].dot(self.weights[l]) + self.biases[l]\n",
    "            activation = self.layers[l].activation_function(z)\n",
    "            a.append(activation)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def backpropagation(self, x,y_mat,a):\n",
    "        m = x.shape[0]\n",
    "        output = a[-1]\n",
    "        \n",
    "        loss = (-1 / m) * np.sum(y_mat * np.log(output))\n",
    "        \n",
    "        deltas = []\n",
    "        delta = y_mat - output\n",
    "        deltas.append(delta)\n",
    "        \n",
    "        for l in range(len(self.layers)-1):\n",
    "            prime = self.layers[-2 - l].function(a[-2 - l])\n",
    "            w = self.weights[-1-l]\n",
    "            delta = np.dot(delta, w.T) * prime\n",
    "            deltas.append(delta)\n",
    "        \n",
    "        prev_weights = self.previous_weights.pop(0)\n",
    "        prev_biases = self.previous_biases.pop(0)\n",
    "\n",
    "        \n",
    "        for l in range(len(self.layers)-1):\n",
    "            dw = (2/m) * np.dot(a[l].T,deltas[-1-l])\n",
    "            self.weights[l] += self.l_rate * dw\n",
    "            \n",
    "            db = (1/m) * np.sum(deltas[-1-l], axis=0, keepdims=True)\n",
    "            self.biases[l] += self.l_rate * db\n",
    "   \n",
    "        self.backpropogation_weights()\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            n_cur_layer_neurons = self.layers[i].n_out\n",
    "            n_prev_layer_neurons = self.layers[i].n_in\n",
    "\n",
    "            weights = self.layers[i].weight_function(n_prev_layer_neurons, n_cur_layer_neurons)\n",
    "            self.weights.append(weights)\n",
    "\n",
    "            biases = np.zeros((1, n_cur_layer_neurons))\n",
    "            self.biases.append(biases)\n",
    "\n",
    "    def train(self, x, y, n_epoch=10000):\n",
    "        \n",
    "        self.initialize_weights()\n",
    "        self.backpropogation_weights()\n",
    "        \n",
    "        y_mat = self.oneHotIt(y)\n",
    "        \n",
    "        for i in range(n_epoch):\n",
    "            a = self.forward_propagation(x)\n",
    "            loss = self.backpropagation(x,y_mat,a)\n",
    "                       \n",
    "            if i%1000==0:\n",
    "                print('Iteration: {0}  --  Loss: {1}'.format(i,loss))\n",
    "                self.losses.append([i,loss])\n",
    "            \n",
    "    #Encode Target Label IDs to one hot vector of size m where m is the number of unique labels\n",
    "    def oneHotIt(self, Y):\n",
    "        m = Y.shape[0]\n",
    "        label = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "        label = np.array(label.todense()).T\n",
    "        return label\n",
    "    \n",
    "    def Predicted_value(self, x):\n",
    "        probs = self.forward_propagation(x)[-1]\n",
    "        preds = np.argmax(probs,axis=1)      \n",
    "        return probs,preds\n",
    "\n",
    "    def Accuracy(self, x,y):\n",
    "        prob,predicted_val = self.Predicted_value(x)\n",
    "        accuracy = sum(predicted_val == y)/(float(len(y)))\n",
    "        percentage = accuracy*100\n",
    "        return percentage\n",
    "    \n",
    "    def loss_graph(self):\n",
    "        errors = np.array(self.losses)\n",
    "        plt.plot(errors[:, 0], errors[:, 1], 'r--')\n",
    "        plt.title(\"(MNIST data) Loss vs Epoch (Learning rate = 0.1)\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0  --  Loss: 2.3446453510897127\n",
      "Iteration: 1000  --  Loss: 2.3445033146973207\n",
      "Iteration: 2000  --  Loss: 2.3365099513521788\n",
      "Iteration: 3000  --  Loss: 2.2973107344388857\n",
      "Iteration: 4000  --  Loss: 2.188965270203473\n",
      "Iteration: 5000  --  Loss: 2.1308601673673477\n",
      "Iteration: 6000  --  Loss: 2.0797318712518362\n",
      "Iteration: 7000  --  Loss: 2.0617118088804403\n",
      "Iteration: 8000  --  Loss: 2.0517631904424256\n",
      "Iteration: 9000  --  Loss: 2.0442793006363567\n",
      "Iteration: 10000  --  Loss: 2.0296736352116325\n",
      "Iteration: 11000  --  Loss: 2.0207333550847557\n",
      "Iteration: 12000  --  Loss: 2.0146353805204016\n",
      "Iteration: 13000  --  Loss: 1.9889714170790358\n",
      "Iteration: 14000  --  Loss: 1.9826319363324205\n",
      "Iteration: 15000  --  Loss: 1.9785517604277758\n",
      "Iteration: 16000  --  Loss: 1.9752583185358348\n",
      "Iteration: 17000  --  Loss: 1.9502953191914139\n",
      "Iteration: 18000  --  Loss: 1.933426569118058\n",
      "Iteration: 19000  --  Loss: 1.9289386622132751\n",
      "Iteration: 20000  --  Loss: 1.9262288735762496\n",
      "Iteration: 21000  --  Loss: 1.9239602405829248\n",
      "Iteration: 22000  --  Loss: 1.9219336002125056\n",
      "Iteration: 23000  --  Loss: 1.9200678546828014\n",
      "Iteration: 24000  --  Loss: 1.9080738745864578\n",
      "Iteration: 25000  --  Loss: 1.8996526027361325\n",
      "Iteration: 26000  --  Loss: 1.8970615462953024\n",
      "Iteration: 27000  --  Loss: 1.8951283067160982\n",
      "Iteration: 28000  --  Loss: 1.893512180896796\n",
      "Iteration: 29000  --  Loss: 1.8920802647142938\n",
      "Iteration: 30000  --  Loss: 1.8906159643194478\n",
      "Iteration: 31000  --  Loss: 1.8891828029921327\n",
      "Iteration: 32000  --  Loss: 1.887889505629863\n",
      "Iteration: 33000  --  Loss: 1.8867994232544998\n",
      "Iteration: 34000  --  Loss: 1.8858092198804637\n",
      "Iteration: 35000  --  Loss: 1.8848892009079132\n",
      "Iteration: 36000  --  Loss: 1.884028877571424\n",
      "Iteration: 37000  --  Loss: 1.8832280224888878\n",
      "Iteration: 38000  --  Loss: 1.8824977753870953\n",
      "Iteration: 39000  --  Loss: 1.8818323062703526\n",
      "Iteration: 40000  --  Loss: 1.8811961729123592\n",
      "Iteration: 41000  --  Loss: 1.8805908064596897\n",
      "Iteration: 42000  --  Loss: 1.879992442338027\n",
      "Iteration: 43000  --  Loss: 1.8698861451599285\n",
      "Iteration: 44000  --  Loss: 1.8687035602949158\n",
      "Iteration: 45000  --  Loss: 1.867937707793131\n",
      "Iteration: 46000  --  Loss: 1.8672507709617707\n",
      "Iteration: 47000  --  Loss: 1.8666407162173388\n",
      "Iteration: 48000  --  Loss: 1.866099430164886\n",
      "Iteration: 49000  --  Loss: 1.8656197456159354\n",
      "Iteration: 50000  --  Loss: 1.865180777769614\n",
      "Iteration: 51000  --  Loss: 1.8646578549529815\n",
      "Iteration: 52000  --  Loss: 1.8642335657586078\n",
      "Iteration: 53000  --  Loss: 1.8638575985010666\n",
      "Iteration: 54000  --  Loss: 1.8634763112047437\n",
      "Iteration: 55000  --  Loss: 1.8630503351668568\n",
      "Iteration: 56000  --  Loss: 1.8626310089782554\n",
      "Iteration: 57000  --  Loss: 1.8622429290576357\n",
      "Iteration: 58000  --  Loss: 1.861780179896868\n",
      "Iteration: 59000  --  Loss: 1.8613968691433047\n",
      "Iteration: 60000  --  Loss: 1.8610523239641126\n",
      "Iteration: 61000  --  Loss: 1.8607073002091026\n",
      "Iteration: 62000  --  Loss: 1.8603874850103608\n",
      "Iteration: 63000  --  Loss: 1.8600449528505658\n",
      "Iteration: 64000  --  Loss: 1.8595369333331602\n",
      "Iteration: 65000  --  Loss: 1.8591709349134273\n",
      "Iteration: 66000  --  Loss: 1.8588077985529334\n",
      "Iteration: 67000  --  Loss: 1.8580891056255024\n",
      "Iteration: 68000  --  Loss: 1.8205630285584165\n",
      "Iteration: 69000  --  Loss: 1.8168211441375501\n",
      "Iteration: 70000  --  Loss: 1.8159198269732988\n",
      "Iteration: 71000  --  Loss: 1.81526953514128\n",
      "Iteration: 72000  --  Loss: 1.7984257484571868\n",
      "Iteration: 73000  --  Loss: 1.7926910458155776\n",
      "Iteration: 74000  --  Loss: 1.7917002726518485\n",
      "Iteration: 75000  --  Loss: 1.7910183216147597\n",
      "Iteration: 76000  --  Loss: 1.7904166923029325\n",
      "Iteration: 77000  --  Loss: 1.789882297331921\n",
      "Iteration: 78000  --  Loss: 1.7894178188364345\n",
      "Iteration: 79000  --  Loss: 1.7890122284707273\n",
      "Iteration: 80000  --  Loss: 1.7886012515881085\n",
      "Iteration: 81000  --  Loss: 1.7881930377526285\n",
      "Iteration: 82000  --  Loss: 1.7878288528036532\n",
      "Iteration: 83000  --  Loss: 1.7874885371443578\n",
      "Iteration: 84000  --  Loss: 1.7871534174388004\n",
      "Iteration: 85000  --  Loss: 1.7867835460387842\n",
      "Iteration: 86000  --  Loss: 1.7864555592326277\n",
      "Iteration: 87000  --  Loss: 1.785418359913823\n",
      "Iteration: 88000  --  Loss: 1.770052608156397\n",
      "Iteration: 89000  --  Loss: 1.7692515185746536\n",
      "Iteration: 90000  --  Loss: 1.7687370251964332\n",
      "Iteration: 91000  --  Loss: 1.7683030274323204\n",
      "Iteration: 92000  --  Loss: 1.767948703406305\n",
      "Iteration: 93000  --  Loss: 1.7676352148374908\n",
      "Iteration: 94000  --  Loss: 1.7673525923536773\n",
      "Iteration: 95000  --  Loss: 1.7670904477249498\n",
      "Iteration: 96000  --  Loss: 1.7668420235485571\n",
      "Iteration: 97000  --  Loss: 1.7666009877308184\n",
      "Iteration: 98000  --  Loss: 1.7663816290545133\n",
      "Iteration: 99000  --  Loss: 1.7661182376940927\n",
      "Iteration: 100000  --  Loss: 1.7658793571447537\n",
      "Iteration: 101000  --  Loss: 1.7656833737254498\n",
      "Iteration: 102000  --  Loss: 1.7655030334960995\n",
      "Iteration: 103000  --  Loss: 1.7653258729013523\n",
      "Iteration: 104000  --  Loss: 1.7651542802174878\n",
      "Iteration: 105000  --  Loss: 1.7649833161783786\n",
      "Iteration: 106000  --  Loss: 1.7647932849315584\n",
      "Iteration: 107000  --  Loss: 1.7645718478682193\n",
      "Iteration: 108000  --  Loss: 1.764370869569331\n",
      "Iteration: 109000  --  Loss: 1.7641723955548938\n",
      "Iteration: 110000  --  Loss: 1.7639257367156065\n",
      "Iteration: 111000  --  Loss: 1.7636886623145642\n",
      "Iteration: 112000  --  Loss: 1.7634578567252557\n",
      "Iteration: 113000  --  Loss: 1.7632780437307547\n",
      "Iteration: 114000  --  Loss: 1.7630627435534951\n",
      "Iteration: 115000  --  Loss: 1.7626504801274379\n",
      "Iteration: 116000  --  Loss: 1.762104686276508\n",
      "Iteration: 117000  --  Loss: 1.7617587953099136\n",
      "Iteration: 118000  --  Loss: 1.7614861941705497\n",
      "Iteration: 119000  --  Loss: 1.7612285011356772\n",
      "Iteration: 120000  --  Loss: 1.761013033596376\n",
      "Iteration: 121000  --  Loss: 1.7608032459309786\n",
      "Iteration: 122000  --  Loss: 1.7605335962736608\n",
      "Iteration: 123000  --  Loss: 1.760213170027871\n",
      "Iteration: 124000  --  Loss: 1.7599826278981887\n",
      "Iteration: 125000  --  Loss: 1.7471049254817255\n",
      "Iteration: 126000  --  Loss: 1.738403600248988\n",
      "Iteration: 127000  --  Loss: 1.7370923496698465\n",
      "Iteration: 128000  --  Loss: 1.7363315138166877\n",
      "Iteration: 129000  --  Loss: 1.7357346943342835\n",
      "Iteration: 130000  --  Loss: 1.7352247855335108\n",
      "Iteration: 131000  --  Loss: 1.7347400251111145\n",
      "Iteration: 132000  --  Loss: 1.7218014210797856\n",
      "Iteration: 133000  --  Loss: 1.6956128955019478\n",
      "Iteration: 134000  --  Loss: 1.6924052950130954\n",
      "Iteration: 135000  --  Loss: 1.691170415093821\n",
      "Iteration: 136000  --  Loss: 1.69033606543085\n",
      "Iteration: 137000  --  Loss: 1.68964243327965\n",
      "Iteration: 138000  --  Loss: 1.6890506364331457\n",
      "Iteration: 139000  --  Loss: 1.6885580814647456\n",
      "Iteration: 140000  --  Loss: 1.6881202567964158\n",
      "Iteration: 141000  --  Loss: 1.6877010744580954\n",
      "Iteration: 142000  --  Loss: 1.687223038922832\n",
      "Iteration: 143000  --  Loss: 1.6868260750809594\n",
      "Iteration: 144000  --  Loss: 1.6864825947919457\n",
      "Iteration: 145000  --  Loss: 1.6861771941433943\n",
      "Iteration: 146000  --  Loss: 1.685830995949863\n",
      "Iteration: 147000  --  Loss: 1.6855283117357192\n",
      "Iteration: 148000  --  Loss: 1.6852622675995397\n",
      "Iteration: 149000  --  Loss: 1.6850170686127424\n",
      "Iteration: 150000  --  Loss: 1.6847916727877412\n",
      "Iteration: 151000  --  Loss: 1.6845577323623495\n",
      "Iteration: 152000  --  Loss: 1.6843089013760457\n",
      "Iteration: 153000  --  Loss: 1.684081606833272\n",
      "Iteration: 154000  --  Loss: 1.6838609419832096\n",
      "Iteration: 155000  --  Loss: 1.6836602184332246\n",
      "Iteration: 156000  --  Loss: 1.6834721280339462\n",
      "Iteration: 157000  --  Loss: 1.6832395140063874\n",
      "Iteration: 158000  --  Loss: 1.6830432433440083\n",
      "Iteration: 159000  --  Loss: 1.6828381314379466\n",
      "Iteration: 160000  --  Loss: 1.6826537781678486\n",
      "Iteration: 161000  --  Loss: 1.6824834328053848\n",
      "Iteration: 162000  --  Loss: 1.6823408478187394\n",
      "Iteration: 163000  --  Loss: 1.6821982835411244\n",
      "Iteration: 164000  --  Loss: 1.6820508566536898\n",
      "Iteration: 165000  --  Loss: 1.681741971176824\n",
      "Iteration: 166000  --  Loss: 1.681453832633515\n",
      "Iteration: 167000  --  Loss: 1.6811847791093777\n",
      "Iteration: 168000  --  Loss: 1.6809173694073334\n",
      "Iteration: 169000  --  Loss: 1.6806661535293763\n",
      "Iteration: 170000  --  Loss: 1.6804420701325482\n",
      "Iteration: 171000  --  Loss: 1.6802472022378376\n",
      "Iteration: 172000  --  Loss: 1.680065166350522\n",
      "Iteration: 173000  --  Loss: 1.6798655472138433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 174000  --  Loss: 1.679626252206125\n",
      "Iteration: 175000  --  Loss: 1.679417867686571\n",
      "Iteration: 176000  --  Loss: 1.6791580091105474\n",
      "Iteration: 177000  --  Loss: 1.6789474104006412\n",
      "Iteration: 178000  --  Loss: 1.6787467554162727\n",
      "Iteration: 179000  --  Loss: 1.6785876566040756\n",
      "Iteration: 180000  --  Loss: 1.6784344268324827\n",
      "Iteration: 181000  --  Loss: 1.6782891931526562\n",
      "Iteration: 182000  --  Loss: 1.6758145979528274\n",
      "Iteration: 183000  --  Loss: 1.6652778326487925\n",
      "Iteration: 184000  --  Loss: 1.6642279605149257\n",
      "Iteration: 185000  --  Loss: 1.6636723038710262\n",
      "Iteration: 186000  --  Loss: 1.663220266186211\n",
      "Iteration: 187000  --  Loss: 1.6627964298977562\n",
      "Iteration: 188000  --  Loss: 1.6624288486565943\n",
      "Iteration: 189000  --  Loss: 1.6621153234166086\n",
      "Iteration: 190000  --  Loss: 1.661839172460695\n",
      "Iteration: 191000  --  Loss: 1.6615305026354814\n",
      "Iteration: 192000  --  Loss: 1.6612896662211774\n",
      "Iteration: 193000  --  Loss: 1.6610846643440746\n",
      "Iteration: 194000  --  Loss: 1.660860566790807\n",
      "Iteration: 195000  --  Loss: 1.6606762191602242\n",
      "Iteration: 196000  --  Loss: 1.660504924344141\n",
      "Iteration: 197000  --  Loss: 1.660344153852039\n",
      "Iteration: 198000  --  Loss: 1.6601714185924143\n",
      "Iteration: 199000  --  Loss: 1.6600108736441652\n",
      "Iteration: 200000  --  Loss: 1.6598130854264004\n",
      "Iteration: 201000  --  Loss: 1.659617509078287\n",
      "Iteration: 202000  --  Loss: 1.6594699255171963\n",
      "Iteration: 203000  --  Loss: 1.6593272480739798\n",
      "Iteration: 204000  --  Loss: 1.6591904747653432\n",
      "Iteration: 205000  --  Loss: 1.6590670147527573\n",
      "Iteration: 206000  --  Loss: 1.6589610448240375\n",
      "Iteration: 207000  --  Loss: 1.6588640977752216\n",
      "Iteration: 208000  --  Loss: 1.6587696750618892\n",
      "Iteration: 209000  --  Loss: 1.6586472856409493\n",
      "Iteration: 210000  --  Loss: 1.658455847720544\n",
      "Iteration: 211000  --  Loss: 1.6583351755125972\n",
      "Iteration: 212000  --  Loss: 1.6582225326256046\n",
      "Iteration: 213000  --  Loss: 1.658121165201066\n",
      "Iteration: 214000  --  Loss: 1.6580330165389956\n",
      "Iteration: 215000  --  Loss: 1.6579523524905733\n",
      "Iteration: 216000  --  Loss: 1.6578745318266508\n",
      "Iteration: 217000  --  Loss: 1.6577960124122368\n",
      "Iteration: 218000  --  Loss: 1.6577182505237822\n",
      "Iteration: 219000  --  Loss: 1.6576439338949018\n",
      "Iteration: 220000  --  Loss: 1.6575588044304228\n",
      "Iteration: 221000  --  Loss: 1.6574619477051937\n",
      "Iteration: 222000  --  Loss: 1.6573875526277138\n",
      "Iteration: 223000  --  Loss: 1.6573232176370551\n",
      "Iteration: 224000  --  Loss: 1.6572643954775181\n",
      "Iteration: 225000  --  Loss: 1.6572087346795625\n",
      "Iteration: 226000  --  Loss: 1.6571541689629303\n",
      "Iteration: 227000  --  Loss: 1.657099206314674\n",
      "Iteration: 228000  --  Loss: 1.6570455209942874\n",
      "Iteration: 229000  --  Loss: 1.6569926536297923\n",
      "Iteration: 230000  --  Loss: 1.6569358032590171\n",
      "Iteration: 231000  --  Loss: 1.6568350235191267\n",
      "Iteration: 232000  --  Loss: 1.656763544234207\n",
      "Iteration: 233000  --  Loss: 1.6567105658617058\n",
      "Iteration: 234000  --  Loss: 1.6566632708429243\n",
      "Iteration: 235000  --  Loss: 1.6566147617843927\n",
      "Iteration: 236000  --  Loss: 1.656528884586245\n",
      "Iteration: 237000  --  Loss: 1.6564497057339393\n",
      "Iteration: 238000  --  Loss: 1.656403033003508\n",
      "Iteration: 239000  --  Loss: 1.6563549119401766\n",
      "Iteration: 240000  --  Loss: 1.6562714883582834\n",
      "Iteration: 241000  --  Loss: 1.6561995719230422\n",
      "Iteration: 242000  --  Loss: 1.6559583997734566\n",
      "Iteration: 243000  --  Loss: 1.6558353802974137\n",
      "Iteration: 244000  --  Loss: 1.6557460437461045\n",
      "Iteration: 245000  --  Loss: 1.6556479305073373\n",
      "Iteration: 246000  --  Loss: 1.6555185998892465\n",
      "Iteration: 247000  --  Loss: 1.6554465912246998\n",
      "Iteration: 248000  --  Loss: 1.6553842157039658\n",
      "Iteration: 249000  --  Loss: 1.6553262675296676\n",
      "Iteration: 250000  --  Loss: 1.655267261727179\n",
      "Iteration: 251000  --  Loss: 1.6551699625797414\n",
      "Iteration: 252000  --  Loss: 1.6550824526128223\n",
      "Iteration: 253000  --  Loss: 1.6549487610515599\n",
      "Iteration: 254000  --  Loss: 1.6548630796474058\n",
      "Iteration: 255000  --  Loss: 1.6548075945421992\n",
      "Iteration: 256000  --  Loss: 1.654754735768199\n",
      "Iteration: 257000  --  Loss: 1.654701285173843\n",
      "Iteration: 258000  --  Loss: 1.654650489449473\n",
      "Iteration: 259000  --  Loss: 1.6545997765401643\n",
      "Iteration: 260000  --  Loss: 1.6545409950421741\n",
      "Iteration: 261000  --  Loss: 1.6544679166272342\n",
      "Iteration: 262000  --  Loss: 1.6544052577735815\n",
      "Iteration: 263000  --  Loss: 1.6542955810647961\n",
      "Iteration: 264000  --  Loss: 1.6542209566484143\n",
      "Iteration: 265000  --  Loss: 1.654107866944666\n",
      "Iteration: 266000  --  Loss: 1.6540513905859302\n",
      "Iteration: 267000  --  Loss: 1.6540008459872049\n",
      "Iteration: 268000  --  Loss: 1.6539187510381308\n",
      "Iteration: 269000  --  Loss: 1.6538525531074497\n",
      "Iteration: 270000  --  Loss: 1.6537904114631814\n",
      "Iteration: 271000  --  Loss: 1.6537091842361422\n",
      "Iteration: 272000  --  Loss: 1.6536516983056218\n",
      "Iteration: 273000  --  Loss: 1.65359148514682\n",
      "Iteration: 274000  --  Loss: 1.6535215373049186\n",
      "Iteration: 275000  --  Loss: 1.6534642795665777\n",
      "Iteration: 276000  --  Loss: 1.6533942785331037\n",
      "Iteration: 277000  --  Loss: 1.6533250013398908\n",
      "Iteration: 278000  --  Loss: 1.6532704515936338\n",
      "Iteration: 279000  --  Loss: 1.6532023446826813\n",
      "Iteration: 280000  --  Loss: 1.6531500601359135\n",
      "Iteration: 281000  --  Loss: 1.6530612326648926\n",
      "Iteration: 282000  --  Loss: 1.653011988979921\n",
      "Iteration: 283000  --  Loss: 1.652969338376046\n",
      "Iteration: 284000  --  Loss: 1.6529279391182\n",
      "Iteration: 285000  --  Loss: 1.6528801755404308\n",
      "Iteration: 286000  --  Loss: 1.652819375539082\n",
      "Iteration: 287000  --  Loss: 1.6527718523154837\n",
      "Iteration: 288000  --  Loss: 1.6527233783840032\n",
      "Iteration: 289000  --  Loss: 1.6526051008262528\n",
      "Iteration: 290000  --  Loss: 1.6525463092319272\n",
      "Iteration: 291000  --  Loss: 1.6524752770864293\n",
      "Iteration: 292000  --  Loss: 1.652428920962112\n",
      "Iteration: 293000  --  Loss: 1.6523816634216564\n",
      "Iteration: 294000  --  Loss: 1.6523261209711313\n",
      "Iteration: 295000  --  Loss: 1.6522561073839301\n",
      "Iteration: 296000  --  Loss: 1.6521984147202344\n",
      "Iteration: 297000  --  Loss: 1.6521352511898153\n",
      "Iteration: 298000  --  Loss: 1.652096442411501\n",
      "Iteration: 299000  --  Loss: 1.6520299611129188\n",
      "Iteration: 300000  --  Loss: 1.6519739019816562\n",
      "Iteration: 301000  --  Loss: 1.6519363339304804\n",
      "Iteration: 302000  --  Loss: 1.6519031778490862\n",
      "Iteration: 303000  --  Loss: 1.6518643581369445\n",
      "Iteration: 304000  --  Loss: 1.6518255045852206\n",
      "Iteration: 305000  --  Loss: 1.65179554087398\n",
      "Iteration: 306000  --  Loss: 1.6517667461178231\n",
      "Iteration: 307000  --  Loss: 1.6517183159156235\n",
      "Iteration: 308000  --  Loss: 1.651677786993175\n",
      "Iteration: 309000  --  Loss: 1.6515617222655579\n",
      "Iteration: 310000  --  Loss: 1.6515131204402955\n",
      "Iteration: 311000  --  Loss: 1.651478503734896\n",
      "Iteration: 312000  --  Loss: 1.6514488684601296\n",
      "Iteration: 313000  --  Loss: 1.6514231318163648\n",
      "Iteration: 314000  --  Loss: 1.6513999100657082\n",
      "Iteration: 315000  --  Loss: 1.6513783054062974\n",
      "Iteration: 316000  --  Loss: 1.6513577294924895\n",
      "Iteration: 317000  --  Loss: 1.6513370235311837\n",
      "Iteration: 318000  --  Loss: 1.651313914378471\n",
      "Iteration: 319000  --  Loss: 1.651292407340109\n",
      "Iteration: 320000  --  Loss: 1.651273026554744\n",
      "Iteration: 321000  --  Loss: 1.6512547978472776\n",
      "Iteration: 322000  --  Loss: 1.6512371266598564\n",
      "Iteration: 323000  --  Loss: 1.6512189964761155\n",
      "Iteration: 324000  --  Loss: 1.651199400779016\n",
      "Iteration: 325000  --  Loss: 1.6511792751599383\n",
      "Iteration: 326000  --  Loss: 1.6511314933680512\n",
      "Iteration: 327000  --  Loss: 1.6510849434826647\n",
      "Iteration: 328000  --  Loss: 1.6510606697250492\n",
      "Iteration: 329000  --  Loss: 1.6510392285317839\n",
      "Iteration: 330000  --  Loss: 1.6509652567229183\n",
      "Iteration: 331000  --  Loss: 1.65092277928323\n",
      "Iteration: 332000  --  Loss: 1.6508665839168626\n",
      "Iteration: 333000  --  Loss: 1.6508403541300298\n",
      "Iteration: 334000  --  Loss: 1.6508203926704934\n",
      "Iteration: 335000  --  Loss: 1.650802713785957\n",
      "Iteration: 336000  --  Loss: 1.6507844107169527\n",
      "Iteration: 337000  --  Loss: 1.6507381075065082\n",
      "Iteration: 338000  --  Loss: 1.6507043310749419\n",
      "Iteration: 339000  --  Loss: 1.6506833534277043\n",
      "Iteration: 340000  --  Loss: 1.6506669265849816\n",
      "Iteration: 341000  --  Loss: 1.6506521139948247\n",
      "Iteration: 342000  --  Loss: 1.6506382979998822\n",
      "Iteration: 343000  --  Loss: 1.6506251680063917\n",
      "Iteration: 344000  --  Loss: 1.650612499809118\n",
      "Iteration: 345000  --  Loss: 1.6506000058426678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 346000  --  Loss: 1.6505865250341198\n",
      "Iteration: 347000  --  Loss: 1.6505327337433677\n",
      "Iteration: 348000  --  Loss: 1.6505054105169696\n",
      "Iteration: 349000  --  Loss: 1.65048621823733\n",
      "Iteration: 350000  --  Loss: 1.6504684656806816\n",
      "Iteration: 351000  --  Loss: 1.6504471711072743\n",
      "Iteration: 352000  --  Loss: 1.6504263882808563\n",
      "Iteration: 353000  --  Loss: 1.6504092946078466\n",
      "Iteration: 354000  --  Loss: 1.6503933082980493\n",
      "Iteration: 355000  --  Loss: 1.6503761121929934\n",
      "Iteration: 356000  --  Loss: 1.6503555844573297\n",
      "Iteration: 357000  --  Loss: 1.6503359388225765\n",
      "Iteration: 358000  --  Loss: 1.650283606747492\n",
      "Iteration: 359000  --  Loss: 1.6502569862294325\n",
      "Iteration: 360000  --  Loss: 1.650239571966855\n",
      "Iteration: 361000  --  Loss: 1.650220622512632\n",
      "Iteration: 362000  --  Loss: 1.6501927830595429\n",
      "Iteration: 363000  --  Loss: 1.6501676137818087\n",
      "Iteration: 364000  --  Loss: 1.6501496915886038\n",
      "Iteration: 365000  --  Loss: 1.650134219329742\n",
      "Iteration: 366000  --  Loss: 1.6501197728522243\n",
      "Iteration: 367000  --  Loss: 1.6501049379534698\n",
      "Iteration: 368000  --  Loss: 1.6500678613598516\n",
      "Iteration: 369000  --  Loss: 1.6500253567174914\n",
      "Iteration: 370000  --  Loss: 1.6499461131325117\n",
      "Iteration: 371000  --  Loss: 1.6499174282281628\n",
      "Iteration: 372000  --  Loss: 1.6498973778543664\n",
      "Iteration: 373000  --  Loss: 1.6498808388422954\n",
      "Iteration: 374000  --  Loss: 1.6498656260810896\n",
      "Iteration: 375000  --  Loss: 1.6498490558790413\n",
      "Iteration: 376000  --  Loss: 1.6497873079954821\n",
      "Iteration: 377000  --  Loss: 1.6497518302940781\n",
      "Iteration: 378000  --  Loss: 1.6497266462244433\n",
      "Iteration: 379000  --  Loss: 1.6496971953988233\n",
      "Iteration: 380000  --  Loss: 1.649681340331055\n",
      "Iteration: 381000  --  Loss: 1.6496663792068471\n",
      "Iteration: 382000  --  Loss: 1.649644878838994\n",
      "Iteration: 383000  --  Loss: 1.6496081380604115\n",
      "Iteration: 384000  --  Loss: 1.6495858338874239\n",
      "Iteration: 385000  --  Loss: 1.6495677145256729\n",
      "Iteration: 386000  --  Loss: 1.6495516413363465\n",
      "Iteration: 387000  --  Loss: 1.6495309038190387\n",
      "Iteration: 388000  --  Loss: 1.6494588614052013\n",
      "Iteration: 389000  --  Loss: 1.6494039247269798\n",
      "Iteration: 390000  --  Loss: 1.6493489543825461\n",
      "Iteration: 391000  --  Loss: 1.649326044146056\n",
      "Iteration: 392000  --  Loss: 1.6492757132730402\n",
      "Iteration: 393000  --  Loss: 1.649250835840029\n",
      "Iteration: 394000  --  Loss: 1.6492347851265177\n",
      "Iteration: 395000  --  Loss: 1.6492213611033022\n",
      "Iteration: 396000  --  Loss: 1.6492093732136113\n",
      "Iteration: 397000  --  Loss: 1.649198384964579\n",
      "Iteration: 398000  --  Loss: 1.6491881536307031\n",
      "Iteration: 399000  --  Loss: 1.6491785031805295\n",
      "Iteration: 400000  --  Loss: 1.6491692836008576\n",
      "Iteration: 401000  --  Loss: 1.6491603698295796\n",
      "Iteration: 402000  --  Loss: 1.6491515725579968\n",
      "Iteration: 403000  --  Loss: 1.6491423249512682\n",
      "Iteration: 404000  --  Loss: 1.6491256857124146\n",
      "Iteration: 405000  --  Loss: 1.649095901430955\n",
      "Iteration: 406000  --  Loss: 1.6490841064467026\n",
      "Iteration: 407000  --  Loss: 1.6490746765435236\n",
      "Iteration: 408000  --  Loss: 1.6490661645482911\n",
      "Iteration: 409000  --  Loss: 1.6490582328928134\n",
      "Iteration: 410000  --  Loss: 1.649050728612479\n",
      "Iteration: 411000  --  Loss: 1.6490435282468525\n",
      "Iteration: 412000  --  Loss: 1.6490365075935218\n",
      "Iteration: 413000  --  Loss: 1.6490294920044863\n",
      "Iteration: 414000  --  Loss: 1.6490221638481404\n",
      "Iteration: 415000  --  Loss: 1.6490144093647379\n",
      "Iteration: 416000  --  Loss: 1.649006744067402\n",
      "Iteration: 417000  --  Loss: 1.648999407607734\n",
      "Iteration: 418000  --  Loss: 1.648992294279211\n",
      "Iteration: 419000  --  Loss: 1.6489852713861566\n",
      "Iteration: 420000  --  Loss: 1.6489782227637846\n",
      "Iteration: 421000  --  Loss: 1.6489709694615888\n",
      "Iteration: 422000  --  Loss: 1.6489628201863458\n",
      "Iteration: 423000  --  Loss: 1.6489485888559645\n",
      "Iteration: 424000  --  Loss: 1.6489123290532\n",
      "Iteration: 425000  --  Loss: 1.6488929100193033\n",
      "Iteration: 426000  --  Loss: 1.6488814132227847\n",
      "Iteration: 427000  --  Loss: 1.6488718614981563\n",
      "Iteration: 428000  --  Loss: 1.6488629316473649\n",
      "Iteration: 429000  --  Loss: 1.6488539688382557\n",
      "Iteration: 430000  --  Loss: 1.6488445114316952\n",
      "Iteration: 431000  --  Loss: 1.6488331144249324\n",
      "Iteration: 432000  --  Loss: 1.6487666080437593\n",
      "Iteration: 433000  --  Loss: 1.6487428869531837\n",
      "Iteration: 434000  --  Loss: 1.6487282442436808\n",
      "Iteration: 435000  --  Loss: 1.6487087746956126\n",
      "Iteration: 436000  --  Loss: 1.648653816732118\n",
      "Iteration: 437000  --  Loss: 1.6486361706095896\n",
      "Iteration: 438000  --  Loss: 1.6486211750295268\n",
      "Iteration: 439000  --  Loss: 1.6485889380877292\n",
      "Iteration: 440000  --  Loss: 1.648578738231383\n",
      "Iteration: 441000  --  Loss: 1.6485701848465317\n",
      "Iteration: 442000  --  Loss: 1.6485623829578113\n",
      "Iteration: 443000  --  Loss: 1.6485550715275064\n",
      "Iteration: 444000  --  Loss: 1.6485481177261856\n",
      "Iteration: 445000  --  Loss: 1.6485414376435534\n",
      "Iteration: 446000  --  Loss: 1.6485349559458964\n",
      "Iteration: 447000  --  Loss: 1.6485285535133818\n",
      "Iteration: 448000  --  Loss: 1.6485218051483035\n",
      "Iteration: 449000  --  Loss: 1.648508342179491\n",
      "Iteration: 450000  --  Loss: 1.6484486773090148\n",
      "Iteration: 451000  --  Loss: 1.6484340481000062\n",
      "Iteration: 452000  --  Loss: 1.6483873271733946\n",
      "Iteration: 453000  --  Loss: 1.648369720862936\n",
      "Iteration: 454000  --  Loss: 1.6483586235581582\n",
      "Iteration: 455000  --  Loss: 1.6483477255874932\n",
      "Iteration: 456000  --  Loss: 1.6483376823322715\n",
      "Iteration: 457000  --  Loss: 1.6483276899242512\n",
      "Iteration: 458000  --  Loss: 1.6483184262788284\n",
      "Iteration: 459000  --  Loss: 1.6483091275516204\n",
      "Iteration: 460000  --  Loss: 1.6482996752544572\n",
      "Iteration: 461000  --  Loss: 1.648290978113472\n",
      "Iteration: 462000  --  Loss: 1.6482825498100795\n",
      "Iteration: 463000  --  Loss: 1.6482727462192435\n",
      "Iteration: 464000  --  Loss: 1.6482594220968212\n",
      "Iteration: 465000  --  Loss: 1.6482467183512408\n",
      "Iteration: 466000  --  Loss: 1.6482355787907073\n",
      "Iteration: 467000  --  Loss: 1.6482263383271742\n",
      "Iteration: 468000  --  Loss: 1.6482182895156658\n",
      "Iteration: 469000  --  Loss: 1.648210999034791\n",
      "Iteration: 470000  --  Loss: 1.6482042632427205\n",
      "Iteration: 471000  --  Loss: 1.648197966408318\n",
      "Iteration: 472000  --  Loss: 1.6481920362080835\n",
      "Iteration: 473000  --  Loss: 1.6481864195049625\n",
      "Iteration: 474000  --  Loss: 1.648181071210552\n",
      "Iteration: 475000  --  Loss: 1.648175947363661\n",
      "Iteration: 476000  --  Loss: 1.648171012790627\n",
      "Iteration: 477000  --  Loss: 1.6481662307767773\n",
      "Iteration: 478000  --  Loss: 1.6481615703184866\n",
      "Iteration: 479000  --  Loss: 1.648157000131361\n",
      "Iteration: 480000  --  Loss: 1.64815247177623\n",
      "Iteration: 481000  --  Loss: 1.6481479210625394\n",
      "Iteration: 482000  --  Loss: 1.6481431806515927\n",
      "Iteration: 483000  --  Loss: 1.648137568019067\n",
      "Iteration: 484000  --  Loss: 1.6481264273684741\n",
      "Iteration: 485000  --  Loss: 1.6481134690671548\n",
      "Iteration: 486000  --  Loss: 1.6481035170482696\n",
      "Iteration: 487000  --  Loss: 1.6480945166941698\n",
      "Iteration: 488000  --  Loss: 1.6480867379229598\n",
      "Iteration: 489000  --  Loss: 1.6480308026204418\n",
      "Iteration: 490000  --  Loss: 1.648012265100735\n",
      "Iteration: 491000  --  Loss: 1.6480039837940434\n",
      "Iteration: 492000  --  Loss: 1.647997406013605\n",
      "Iteration: 493000  --  Loss: 1.647991561269184\n",
      "Iteration: 494000  --  Loss: 1.647986135445339\n",
      "Iteration: 495000  --  Loss: 1.6479809430926315\n",
      "Iteration: 496000  --  Loss: 1.6479756576203548\n",
      "Iteration: 497000  --  Loss: 1.6479650568739348\n",
      "Iteration: 498000  --  Loss: 1.6478764517130253\n",
      "Iteration: 499000  --  Loss: 1.6478663248234302\n",
      "Iteration: 500000  --  Loss: 1.6478593098815273\n",
      "Iteration: 501000  --  Loss: 1.6478532583189638\n",
      "Iteration: 502000  --  Loss: 1.6478476361824275\n",
      "Iteration: 503000  --  Loss: 1.6478421546658784\n",
      "Iteration: 504000  --  Loss: 1.6478364521168094\n",
      "Iteration: 505000  --  Loss: 1.6478295269668066\n",
      "Iteration: 506000  --  Loss: 1.6478228043046395\n",
      "Iteration: 507000  --  Loss: 1.6478174851654563\n",
      "Iteration: 508000  --  Loss: 1.647812672440049\n",
      "Iteration: 509000  --  Loss: 1.647808065455266\n",
      "Iteration: 510000  --  Loss: 1.6478034912596518\n",
      "Iteration: 511000  --  Loss: 1.647798711940187\n",
      "Iteration: 512000  --  Loss: 1.6477932348240256\n",
      "Iteration: 513000  --  Loss: 1.6477860703522282\n",
      "Iteration: 514000  --  Loss: 1.6477727767164738\n",
      "Iteration: 515000  --  Loss: 1.6477417766716687\n",
      "Iteration: 516000  --  Loss: 1.647663776859998\n",
      "Iteration: 517000  --  Loss: 1.6476401353239107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 518000  --  Loss: 1.6476234541950538\n",
      "Iteration: 519000  --  Loss: 1.6476103064025525\n",
      "Iteration: 520000  --  Loss: 1.6476004250744076\n",
      "Iteration: 521000  --  Loss: 1.647592504830633\n",
      "Iteration: 522000  --  Loss: 1.647585758582177\n",
      "Iteration: 523000  --  Loss: 1.6475797552749551\n",
      "Iteration: 524000  --  Loss: 1.6475742751288287\n",
      "Iteration: 525000  --  Loss: 1.6475691864453583\n",
      "Iteration: 526000  --  Loss: 1.6475644025551814\n",
      "Iteration: 527000  --  Loss: 1.6475598680793002\n",
      "Iteration: 528000  --  Loss: 1.6475555388537186\n",
      "Iteration: 529000  --  Loss: 1.6475513741299632\n",
      "Iteration: 530000  --  Loss: 1.64754732590922\n",
      "Iteration: 531000  --  Loss: 1.6475432968906039\n",
      "Iteration: 532000  --  Loss: 1.6475388275376048\n",
      "Iteration: 533000  --  Loss: 1.6474894008014778\n",
      "Iteration: 534000  --  Loss: 1.6474709755805281\n",
      "Iteration: 535000  --  Loss: 1.647465011353433\n",
      "Iteration: 536000  --  Loss: 1.6474604585394994\n",
      "Iteration: 537000  --  Loss: 1.6474564744103148\n",
      "Iteration: 538000  --  Loss: 1.6474528000330275\n",
      "Iteration: 539000  --  Loss: 1.6474493256325045\n",
      "Iteration: 540000  --  Loss: 1.6474459949150697\n",
      "Iteration: 541000  --  Loss: 1.6474427801080802\n",
      "Iteration: 542000  --  Loss: 1.647439658202636\n",
      "Iteration: 543000  --  Loss: 1.6474366091502375\n",
      "Iteration: 544000  --  Loss: 1.6474336254869077\n",
      "Iteration: 545000  --  Loss: 1.6474306991632508\n",
      "Iteration: 546000  --  Loss: 1.6474278190977105\n",
      "Iteration: 547000  --  Loss: 1.6474249782681158\n",
      "Iteration: 548000  --  Loss: 1.6474221637565825\n",
      "Iteration: 549000  --  Loss: 1.6474193648563544\n",
      "Iteration: 550000  --  Loss: 1.647416557556145\n",
      "Iteration: 551000  --  Loss: 1.6474136785813178\n",
      "Iteration: 552000  --  Loss: 1.6474104405704075\n",
      "Iteration: 553000  --  Loss: 1.6473784759807037\n",
      "Iteration: 554000  --  Loss: 1.647302291361164\n",
      "Iteration: 555000  --  Loss: 1.647296392462502\n",
      "Iteration: 556000  --  Loss: 1.6472922561024812\n",
      "Iteration: 557000  --  Loss: 1.647288336195953\n",
      "Iteration: 558000  --  Loss: 1.6472803184389997\n",
      "Iteration: 559000  --  Loss: 1.6472283601617164\n",
      "Iteration: 560000  --  Loss: 1.6471800800782304\n",
      "Iteration: 561000  --  Loss: 1.6471709672607533\n",
      "Iteration: 562000  --  Loss: 1.6471650486645877\n",
      "Iteration: 563000  --  Loss: 1.647159588037036\n",
      "Iteration: 564000  --  Loss: 1.647128266872275\n",
      "Iteration: 565000  --  Loss: 1.6471201006232068\n",
      "Iteration: 566000  --  Loss: 1.6471154464012385\n",
      "Iteration: 567000  --  Loss: 1.6471112359822104\n",
      "Iteration: 568000  --  Loss: 1.6471070299729282\n",
      "Iteration: 569000  --  Loss: 1.6471019095451016\n",
      "Iteration: 570000  --  Loss: 1.6470852091247656\n",
      "Iteration: 571000  --  Loss: 1.6470254568859426\n",
      "Iteration: 572000  --  Loss: 1.6470060709958412\n",
      "Iteration: 573000  --  Loss: 1.6469484831743209\n",
      "Iteration: 574000  --  Loss: 1.6468983357952038\n",
      "Iteration: 575000  --  Loss: 1.6468835895727438\n",
      "Iteration: 576000  --  Loss: 1.6468741259537873\n",
      "Iteration: 577000  --  Loss: 1.6468668643887203\n",
      "Iteration: 578000  --  Loss: 1.646860665511926\n",
      "Iteration: 579000  --  Loss: 1.6468540684377668\n",
      "Iteration: 580000  --  Loss: 1.6468179208528373\n",
      "Iteration: 581000  --  Loss: 1.6468068889252232\n",
      "Iteration: 582000  --  Loss: 1.6468007729518235\n",
      "Iteration: 583000  --  Loss: 1.6467957910337432\n",
      "Iteration: 584000  --  Loss: 1.6467913703125954\n",
      "Iteration: 585000  --  Loss: 1.6467873166574192\n",
      "Iteration: 586000  --  Loss: 1.646783535493912\n",
      "Iteration: 587000  --  Loss: 1.6467799650250718\n",
      "Iteration: 588000  --  Loss: 1.646776566467116\n",
      "Iteration: 589000  --  Loss: 1.6467733083057283\n",
      "Iteration: 590000  --  Loss: 1.6467701620769977\n",
      "Iteration: 591000  --  Loss: 1.6467670978066913\n",
      "Iteration: 592000  --  Loss: 1.646764084746775\n",
      "Iteration: 593000  --  Loss: 1.646761070906872\n",
      "Iteration: 594000  --  Loss: 1.646757952765555\n",
      "Iteration: 595000  --  Loss: 1.6467543420375519\n",
      "Iteration: 596000  --  Loss: 1.646739455493136\n",
      "Iteration: 597000  --  Loss: 1.6467047752061545\n",
      "Iteration: 598000  --  Loss: 1.646699085395875\n",
      "Iteration: 599000  --  Loss: 1.6466953311656993\n",
      "Iteration: 600000  --  Loss: 1.6466921995047465\n",
      "Iteration: 601000  --  Loss: 1.6466893553609967\n",
      "Iteration: 602000  --  Loss: 1.6466866788371868\n",
      "Iteration: 603000  --  Loss: 1.6466841015499614\n",
      "Iteration: 604000  --  Loss: 1.6466815801625596\n",
      "Iteration: 605000  --  Loss: 1.6466790329350796\n",
      "Iteration: 606000  --  Loss: 1.646676245402989\n",
      "Iteration: 607000  --  Loss: 1.6466719666234573\n",
      "Iteration: 608000  --  Loss: 1.64665864194536\n",
      "Iteration: 609000  --  Loss: 1.646650078802777\n",
      "Iteration: 610000  --  Loss: 1.6466458413172294\n",
      "Iteration: 611000  --  Loss: 1.6466425004815168\n",
      "Iteration: 612000  --  Loss: 1.646639565710876\n",
      "Iteration: 613000  --  Loss: 1.646636868241732\n",
      "Iteration: 614000  --  Loss: 1.64663432986347\n",
      "Iteration: 615000  --  Loss: 1.6466319019816602\n",
      "Iteration: 616000  --  Loss: 1.6466295502735415\n",
      "Iteration: 617000  --  Loss: 1.6466272528228183\n",
      "Iteration: 618000  --  Loss: 1.6466249804393498\n",
      "Iteration: 619000  --  Loss: 1.6466226985766277\n",
      "Iteration: 620000  --  Loss: 1.6466203539508233\n",
      "Iteration: 621000  --  Loss: 1.6466178715633468\n",
      "Iteration: 622000  --  Loss: 1.6466151138234173\n",
      "Iteration: 623000  --  Loss: 1.6466117846867734\n",
      "Iteration: 624000  --  Loss: 1.6466069574036617\n",
      "Iteration: 625000  --  Loss: 1.6465957454056557\n",
      "Iteration: 626000  --  Loss: 1.6465621710365659\n",
      "Iteration: 627000  --  Loss: 1.646547194555033\n",
      "Iteration: 628000  --  Loss: 1.646539379270344\n",
      "Iteration: 629000  --  Loss: 1.6465334695050406\n",
      "Iteration: 630000  --  Loss: 1.6465283958530441\n",
      "Iteration: 631000  --  Loss: 1.6465236344130088\n",
      "Iteration: 632000  --  Loss: 1.646515911945789\n",
      "Iteration: 633000  --  Loss: 1.6464912416481885\n",
      "Iteration: 634000  --  Loss: 1.6464858375107678\n",
      "Iteration: 635000  --  Loss: 1.6464812777492743\n",
      "Iteration: 636000  --  Loss: 1.6464751777515108\n",
      "Iteration: 637000  --  Loss: 1.6464627530635179\n",
      "Iteration: 638000  --  Loss: 1.6464402927374318\n",
      "Iteration: 639000  --  Loss: 1.6464279415857834\n",
      "Iteration: 640000  --  Loss: 1.6464189211803024\n",
      "Iteration: 641000  --  Loss: 1.6464106913537258\n",
      "Iteration: 642000  --  Loss: 1.646404513035142\n",
      "Iteration: 643000  --  Loss: 1.6463993940762638\n",
      "Iteration: 644000  --  Loss: 1.646393837421242\n",
      "Iteration: 645000  --  Loss: 1.646385846732167\n",
      "Iteration: 646000  --  Loss: 1.6463798419214566\n",
      "Iteration: 647000  --  Loss: 1.6463749173559057\n",
      "Iteration: 648000  --  Loss: 1.6463681098479774\n",
      "Iteration: 649000  --  Loss: 1.646331355181182\n",
      "Iteration: 650000  --  Loss: 1.646308176973838\n",
      "Iteration: 651000  --  Loss: 1.6462962825550187\n",
      "Iteration: 652000  --  Loss: 1.6462372930314628\n",
      "Iteration: 653000  --  Loss: 1.6462211045403852\n",
      "Iteration: 654000  --  Loss: 1.6462137688700786\n",
      "Iteration: 655000  --  Loss: 1.6462086520109347\n",
      "Iteration: 656000  --  Loss: 1.64620450027591\n",
      "Iteration: 657000  --  Loss: 1.6462008860328783\n",
      "Iteration: 658000  --  Loss: 1.6461976080219685\n",
      "Iteration: 659000  --  Loss: 1.6461945727367728\n",
      "Iteration: 660000  --  Loss: 1.6461917058450308\n",
      "Iteration: 661000  --  Loss: 1.6461889569004426\n",
      "Iteration: 662000  --  Loss: 1.6461862884794527\n",
      "Iteration: 663000  --  Loss: 1.6461836633226477\n",
      "Iteration: 664000  --  Loss: 1.646181033719278\n",
      "Iteration: 665000  --  Loss: 1.6461783353834822\n",
      "Iteration: 666000  --  Loss: 1.6461754256546477\n",
      "Iteration: 667000  --  Loss: 1.6461716743316572\n",
      "Iteration: 668000  --  Loss: 1.6461634846621551\n",
      "Iteration: 669000  --  Loss: 1.6461389176691106\n",
      "Iteration: 670000  --  Loss: 1.6461327437642077\n",
      "Iteration: 671000  --  Loss: 1.6461288348448002\n",
      "Iteration: 672000  --  Loss: 1.646125611171076\n",
      "Iteration: 673000  --  Loss: 1.64612271421158\n",
      "Iteration: 674000  --  Loss: 1.6461199995137987\n",
      "Iteration: 675000  --  Loss: 1.6461173926328525\n",
      "Iteration: 676000  --  Loss: 1.6461148374121968\n",
      "Iteration: 677000  --  Loss: 1.6461122970911268\n",
      "Iteration: 678000  --  Loss: 1.6461097341311626\n",
      "Iteration: 679000  --  Loss: 1.6461070869298755\n",
      "Iteration: 680000  --  Loss: 1.646104205888638\n",
      "Iteration: 681000  --  Loss: 1.6461003431975179\n",
      "Iteration: 682000  --  Loss: 1.646086342125338\n",
      "Iteration: 683000  --  Loss: 1.6460705041988062\n",
      "Iteration: 684000  --  Loss: 1.6460617068953394\n",
      "Iteration: 685000  --  Loss: 1.6460548088123477\n",
      "Iteration: 686000  --  Loss: 1.6460505184851855\n",
      "Iteration: 687000  --  Loss: 1.646046386186916\n",
      "Iteration: 688000  --  Loss: 1.6460423532369524\n",
      "Iteration: 689000  --  Loss: 1.64603863116271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 690000  --  Loss: 1.6460344658966173\n",
      "Iteration: 691000  --  Loss: 1.6460274576561096\n",
      "Iteration: 692000  --  Loss: 1.6460192642204028\n",
      "Iteration: 693000  --  Loss: 1.6460137796384027\n",
      "Iteration: 694000  --  Loss: 1.6459928916579376\n",
      "Iteration: 695000  --  Loss: 1.6459649911744914\n",
      "Iteration: 696000  --  Loss: 1.645951275773155\n",
      "Iteration: 697000  --  Loss: 1.6459408541530993\n",
      "Iteration: 698000  --  Loss: 1.6459335201703658\n",
      "Iteration: 699000  --  Loss: 1.6459284902122469\n",
      "Iteration: 700000  --  Loss: 1.6459239172526572\n",
      "Iteration: 701000  --  Loss: 1.6459109485639387\n",
      "Iteration: 702000  --  Loss: 1.6458888789067783\n",
      "Iteration: 703000  --  Loss: 1.645881011212623\n",
      "Iteration: 704000  --  Loss: 1.645868978579821\n",
      "Iteration: 705000  --  Loss: 1.6458631707056173\n",
      "Iteration: 706000  --  Loss: 1.6458589611645007\n",
      "Iteration: 707000  --  Loss: 1.6458554275378778\n",
      "Iteration: 708000  --  Loss: 1.6458522816933507\n",
      "Iteration: 709000  --  Loss: 1.6458493930642872\n",
      "Iteration: 710000  --  Loss: 1.6458466792597926\n",
      "Iteration: 711000  --  Loss: 1.6458440787676523\n",
      "Iteration: 712000  --  Loss: 1.645841534778594\n",
      "Iteration: 713000  --  Loss: 1.6458389567268314\n",
      "Iteration: 714000  --  Loss: 1.645836056320743\n",
      "Iteration: 715000  --  Loss: 1.645831963160418\n",
      "Iteration: 716000  --  Loss: 1.6458267790840082\n",
      "Iteration: 717000  --  Loss: 1.6458226226153145\n",
      "Iteration: 718000  --  Loss: 1.6458186941599093\n",
      "Iteration: 719000  --  Loss: 1.6458112484985037\n",
      "Iteration: 720000  --  Loss: 1.6458029991084049\n",
      "Iteration: 721000  --  Loss: 1.6457984692058036\n",
      "Iteration: 722000  --  Loss: 1.6457950002600519\n",
      "Iteration: 723000  --  Loss: 1.6457920437281228\n",
      "Iteration: 724000  --  Loss: 1.6457893857149728\n",
      "Iteration: 725000  --  Loss: 1.6457869338147326\n",
      "Iteration: 726000  --  Loss: 1.6457846385051562\n",
      "Iteration: 727000  --  Loss: 1.6457824680323534\n",
      "Iteration: 728000  --  Loss: 1.645780397867045\n",
      "Iteration: 729000  --  Loss: 1.645778412755077\n",
      "Iteration: 730000  --  Loss: 1.645776499642823\n",
      "Iteration: 731000  --  Loss: 1.6457746488175966\n",
      "Iteration: 732000  --  Loss: 1.6457728505489537\n",
      "Iteration: 733000  --  Loss: 1.6457710979025528\n",
      "Iteration: 734000  --  Loss: 1.645769384688852\n",
      "Iteration: 735000  --  Loss: 1.6457677026939426\n",
      "Iteration: 736000  --  Loss: 1.6457660434319856\n",
      "Iteration: 737000  --  Loss: 1.6457643886642987\n",
      "Iteration: 738000  --  Loss: 1.6457626987794038\n",
      "Iteration: 739000  --  Loss: 1.6457608262762642\n",
      "Iteration: 740000  --  Loss: 1.6457576973671326\n",
      "Iteration: 741000  --  Loss: 1.6457500106836336\n",
      "Iteration: 742000  --  Loss: 1.6457458569751726\n",
      "Iteration: 743000  --  Loss: 1.6457431291652658\n",
      "Iteration: 744000  --  Loss: 1.6457408511373324\n",
      "Iteration: 745000  --  Loss: 1.6457388200749126\n",
      "Iteration: 746000  --  Loss: 1.6457369512817484\n",
      "Iteration: 747000  --  Loss: 1.6457351946478995\n",
      "Iteration: 748000  --  Loss: 1.6457335220983218\n",
      "Iteration: 749000  --  Loss: 1.645731914975581\n",
      "Iteration: 750000  --  Loss: 1.6457303581806755\n",
      "Iteration: 751000  --  Loss: 1.6457288406400747\n",
      "Iteration: 752000  --  Loss: 1.6457273540663617\n",
      "Iteration: 753000  --  Loss: 1.6457258874780696\n",
      "Iteration: 754000  --  Loss: 1.6457244359337795\n",
      "Iteration: 755000  --  Loss: 1.6457229910101123\n",
      "Iteration: 756000  --  Loss: 1.645721541355301\n",
      "Iteration: 757000  --  Loss: 1.6457200707917796\n",
      "Iteration: 758000  --  Loss: 1.6457185538407422\n",
      "Iteration: 759000  --  Loss: 1.645716931132797\n",
      "Iteration: 760000  --  Loss: 1.6457150041658923\n",
      "Iteration: 761000  --  Loss: 1.645711798222085\n",
      "Iteration: 762000  --  Loss: 1.6457049432006023\n",
      "Iteration: 763000  --  Loss: 1.6456998859678174\n",
      "Iteration: 764000  --  Loss: 1.6456962476756076\n",
      "Iteration: 765000  --  Loss: 1.645692903554696\n",
      "Iteration: 766000  --  Loss: 1.6456892339533225\n",
      "Iteration: 767000  --  Loss: 1.6456842558212261\n",
      "Iteration: 768000  --  Loss: 1.6456742693643767\n",
      "Iteration: 769000  --  Loss: 1.645381764555749\n",
      "Iteration: 770000  --  Loss: 1.6261650731545871\n",
      "Iteration: 771000  --  Loss: 1.6252822889693963\n",
      "Iteration: 772000  --  Loss: 1.6248536894057288\n",
      "Iteration: 773000  --  Loss: 1.6245537333001157\n",
      "Iteration: 774000  --  Loss: 1.624328898394781\n",
      "Iteration: 775000  --  Loss: 1.6241479306473177\n",
      "Iteration: 776000  --  Loss: 1.6239926340052926\n",
      "Iteration: 777000  --  Loss: 1.623857947586146\n",
      "Iteration: 778000  --  Loss: 1.6237499021383683\n",
      "Iteration: 779000  --  Loss: 1.6236603096020876\n",
      "Iteration: 780000  --  Loss: 1.623577662541259\n",
      "Iteration: 781000  --  Loss: 1.6234940606942903\n",
      "Iteration: 782000  --  Loss: 1.6234273682133986\n",
      "Iteration: 783000  --  Loss: 1.6233717596454815\n",
      "Iteration: 784000  --  Loss: 1.6233214766243504\n",
      "Iteration: 785000  --  Loss: 1.623273034829619\n",
      "Iteration: 786000  --  Loss: 1.623226553988771\n",
      "Iteration: 787000  --  Loss: 1.6231828204536032\n",
      "Iteration: 788000  --  Loss: 1.6231389043860338\n",
      "Iteration: 789000  --  Loss: 1.6230965293462745\n",
      "Iteration: 790000  --  Loss: 1.6230591171986304\n",
      "Iteration: 791000  --  Loss: 1.623023310731018\n",
      "Iteration: 792000  --  Loss: 1.6229802463918634\n",
      "Iteration: 793000  --  Loss: 1.6229476636079412\n",
      "Iteration: 794000  --  Loss: 1.6229156305077528\n",
      "Iteration: 795000  --  Loss: 1.6228867412138177\n",
      "Iteration: 796000  --  Loss: 1.6228270013567276\n",
      "Iteration: 797000  --  Loss: 1.6227763374594364\n",
      "Iteration: 798000  --  Loss: 1.6227412870460134\n",
      "Iteration: 799000  --  Loss: 1.622718319537081\n",
      "Iteration: 800000  --  Loss: 1.6226983754599082\n",
      "Iteration: 801000  --  Loss: 1.622680432153673\n",
      "Iteration: 802000  --  Loss: 1.6226640482749497\n",
      "Iteration: 803000  --  Loss: 1.6226489477927049\n",
      "Iteration: 804000  --  Loss: 1.6226349284678272\n",
      "Iteration: 805000  --  Loss: 1.6226218536868493\n",
      "Iteration: 806000  --  Loss: 1.622609604961771\n",
      "Iteration: 807000  --  Loss: 1.6225980812549279\n",
      "Iteration: 808000  --  Loss: 1.6225871892573887\n",
      "Iteration: 809000  --  Loss: 1.6225768342032494\n",
      "Iteration: 810000  --  Loss: 1.6225669089506298\n",
      "Iteration: 811000  --  Loss: 1.6225572872113445\n",
      "Iteration: 812000  --  Loss: 1.62254760665582\n",
      "Iteration: 813000  --  Loss: 1.622536092062504\n",
      "Iteration: 814000  --  Loss: 1.622520041798108\n",
      "Iteration: 815000  --  Loss: 1.6225072999902233\n",
      "Iteration: 816000  --  Loss: 1.6224940685681568\n",
      "Iteration: 817000  --  Loss: 1.6224775946592263\n",
      "Iteration: 818000  --  Loss: 1.6224645460395157\n",
      "Iteration: 819000  --  Loss: 1.6224368279861063\n",
      "Iteration: 820000  --  Loss: 1.6224245935824995\n",
      "Iteration: 821000  --  Loss: 1.622415254753492\n",
      "Iteration: 822000  --  Loss: 1.6224069552230809\n",
      "Iteration: 823000  --  Loss: 1.622399278501081\n",
      "Iteration: 824000  --  Loss: 1.6223914068211853\n",
      "Iteration: 825000  --  Loss: 1.6223500329934495\n",
      "Iteration: 826000  --  Loss: 1.6223376114543224\n",
      "Iteration: 827000  --  Loss: 1.622329478091466\n",
      "Iteration: 828000  --  Loss: 1.6223226301323959\n",
      "Iteration: 829000  --  Loss: 1.6223164311730396\n",
      "Iteration: 830000  --  Loss: 1.6223106457182381\n",
      "Iteration: 831000  --  Loss: 1.622305145817692\n",
      "Iteration: 832000  --  Loss: 1.6222998540986662\n",
      "Iteration: 833000  --  Loss: 1.622294683569554\n",
      "Iteration: 834000  --  Loss: 1.6222894806717945\n",
      "Iteration: 835000  --  Loss: 1.6222835279763659\n",
      "Iteration: 836000  --  Loss: 1.6222715969826957\n",
      "Iteration: 837000  --  Loss: 1.6222625115113387\n",
      "Iteration: 838000  --  Loss: 1.6222565686482455\n",
      "Iteration: 839000  --  Loss: 1.6222510765113256\n",
      "Iteration: 840000  --  Loss: 1.6222451427593485\n",
      "Iteration: 841000  --  Loss: 1.6222313775262789\n",
      "Iteration: 842000  --  Loss: 1.6221778450135418\n",
      "Iteration: 843000  --  Loss: 1.6221626907870559\n",
      "Iteration: 844000  --  Loss: 1.6221531550673407\n",
      "Iteration: 845000  --  Loss: 1.6221456060921844\n",
      "Iteration: 846000  --  Loss: 1.622139066514406\n",
      "Iteration: 847000  --  Loss: 1.6221330341717675\n",
      "Iteration: 848000  --  Loss: 1.622127119603875\n",
      "Iteration: 849000  --  Loss: 1.6221199290984172\n",
      "Iteration: 850000  --  Loss: 1.6220667730807268\n",
      "Iteration: 851000  --  Loss: 1.6220420119280956\n",
      "Iteration: 852000  --  Loss: 1.622032675824084\n",
      "Iteration: 853000  --  Loss: 1.6220258805627854\n",
      "Iteration: 854000  --  Loss: 1.6220201214300294\n",
      "Iteration: 855000  --  Loss: 1.6220149180483419\n",
      "Iteration: 856000  --  Loss: 1.6220100502404864\n",
      "Iteration: 857000  --  Loss: 1.6220054014544774\n",
      "Iteration: 858000  --  Loss: 1.6220009174172698\n",
      "Iteration: 859000  --  Loss: 1.6219965694331608\n",
      "Iteration: 860000  --  Loss: 1.6219923250434582\n",
      "Iteration: 861000  --  Loss: 1.6219881547783852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 862000  --  Loss: 1.6219839823026612\n",
      "Iteration: 863000  --  Loss: 1.6219793847051682\n",
      "Iteration: 864000  --  Loss: 1.6219429684609465\n",
      "Iteration: 865000  --  Loss: 1.6219062143208038\n",
      "Iteration: 866000  --  Loss: 1.6218976197068737\n",
      "Iteration: 867000  --  Loss: 1.6218915050654472\n",
      "Iteration: 868000  --  Loss: 1.6218863817617122\n",
      "Iteration: 869000  --  Loss: 1.621881806980642\n",
      "Iteration: 870000  --  Loss: 1.6218775812436672\n",
      "Iteration: 871000  --  Loss: 1.6218735972228837\n",
      "Iteration: 872000  --  Loss: 1.6218697781805023\n",
      "Iteration: 873000  --  Loss: 1.6218660614487042\n",
      "Iteration: 874000  --  Loss: 1.621862407877711\n",
      "Iteration: 875000  --  Loss: 1.6218587890653189\n",
      "Iteration: 876000  --  Loss: 1.621855187410631\n",
      "Iteration: 877000  --  Loss: 1.6218516030999062\n",
      "Iteration: 878000  --  Loss: 1.6218480352495093\n",
      "Iteration: 879000  --  Loss: 1.6218444716253584\n",
      "Iteration: 880000  --  Loss: 1.6218408610049486\n",
      "Iteration: 881000  --  Loss: 1.6218370682828966\n",
      "Iteration: 882000  --  Loss: 1.621832523639033\n",
      "Iteration: 883000  --  Loss: 1.6218195679311544\n",
      "Iteration: 884000  --  Loss: 1.6217173136043506\n",
      "Iteration: 885000  --  Loss: 1.6217051888484604\n",
      "Iteration: 886000  --  Loss: 1.6216988229427465\n",
      "Iteration: 887000  --  Loss: 1.6216937431144436\n",
      "Iteration: 888000  --  Loss: 1.6216891811908163\n",
      "Iteration: 889000  --  Loss: 1.6216847797616167\n",
      "Iteration: 890000  --  Loss: 1.6216798953952516\n",
      "Iteration: 891000  --  Loss: 1.6216643967408693\n",
      "Iteration: 892000  --  Loss: 1.6216012197910055\n",
      "Iteration: 893000  --  Loss: 1.6215910490699939\n",
      "Iteration: 894000  --  Loss: 1.6215853957005553\n",
      "Iteration: 895000  --  Loss: 1.6215809680024498\n",
      "Iteration: 896000  --  Loss: 1.6215771029480555\n",
      "Iteration: 897000  --  Loss: 1.621573576506476\n",
      "Iteration: 898000  --  Loss: 1.6215702835550547\n",
      "Iteration: 899000  --  Loss: 1.62156715843915\n",
      "Iteration: 900000  --  Loss: 1.6215641500555997\n",
      "Iteration: 901000  --  Loss: 1.6215611875348825\n",
      "Iteration: 902000  --  Loss: 1.621558060254044\n",
      "Iteration: 903000  --  Loss: 1.6215524945912243\n",
      "Iteration: 904000  --  Loss: 1.6215354708868104\n",
      "Iteration: 905000  --  Loss: 1.6215303389155309\n",
      "Iteration: 906000  --  Loss: 1.6215269873807066\n",
      "Iteration: 907000  --  Loss: 1.6215241313677868\n",
      "Iteration: 908000  --  Loss: 1.6215215110015488\n",
      "Iteration: 909000  --  Loss: 1.6215190326116777\n",
      "Iteration: 910000  --  Loss: 1.6215166504133987\n",
      "Iteration: 911000  --  Loss: 1.6215143383022113\n",
      "Iteration: 912000  --  Loss: 1.6215120764447588\n",
      "Iteration: 913000  --  Loss: 1.6215098489374205\n",
      "Iteration: 914000  --  Loss: 1.6215076337616412\n",
      "Iteration: 915000  --  Loss: 1.6215054095067836\n",
      "Iteration: 916000  --  Loss: 1.6215031312826862\n",
      "Iteration: 917000  --  Loss: 1.621500648955048\n",
      "Iteration: 918000  --  Loss: 1.6214950709399787\n",
      "Iteration: 919000  --  Loss: 1.6214827276427957\n",
      "Iteration: 920000  --  Loss: 1.6214789210199247\n",
      "Iteration: 921000  --  Loss: 1.6214759131900638\n",
      "Iteration: 922000  --  Loss: 1.6214732006746957\n",
      "Iteration: 923000  --  Loss: 1.621470622341146\n",
      "Iteration: 924000  --  Loss: 1.6214680959946843\n",
      "Iteration: 925000  --  Loss: 1.6214655526588873\n",
      "Iteration: 926000  --  Loss: 1.6214628546904453\n",
      "Iteration: 927000  --  Loss: 1.6214594512766143\n",
      "Iteration: 928000  --  Loss: 1.6214482213973131\n",
      "Iteration: 929000  --  Loss: 1.6214259508681335\n",
      "Iteration: 930000  --  Loss: 1.6214014433539237\n",
      "Iteration: 931000  --  Loss: 1.621391237226269\n",
      "Iteration: 932000  --  Loss: 1.6213861213494316\n",
      "Iteration: 933000  --  Loss: 1.621382258704937\n",
      "Iteration: 934000  --  Loss: 1.621378992039055\n",
      "Iteration: 935000  --  Loss: 1.6213760642684538\n",
      "Iteration: 936000  --  Loss: 1.6213733239073873\n",
      "Iteration: 937000  --  Loss: 1.621370645623489\n",
      "Iteration: 938000  --  Loss: 1.6213678697707223\n",
      "Iteration: 939000  --  Loss: 1.6213646114514333\n",
      "Iteration: 940000  --  Loss: 1.6213590399647333\n",
      "Iteration: 941000  --  Loss: 1.621341983278842\n",
      "Iteration: 942000  --  Loss: 1.6213166506208123\n",
      "Iteration: 943000  --  Loss: 1.6212866737933374\n",
      "Iteration: 944000  --  Loss: 1.621277382205506\n",
      "Iteration: 945000  --  Loss: 1.6212718645179969\n",
      "Iteration: 946000  --  Loss: 1.6212677634603905\n",
      "Iteration: 947000  --  Loss: 1.6212644013527568\n",
      "Iteration: 948000  --  Loss: 1.6212614670281114\n",
      "Iteration: 949000  --  Loss: 1.6212587865728878\n",
      "Iteration: 950000  --  Loss: 1.6212562481851247\n",
      "Iteration: 951000  --  Loss: 1.6212537421458733\n",
      "Iteration: 952000  --  Loss: 1.621251111821624\n",
      "Iteration: 953000  --  Loss: 1.6212479837898044\n",
      "Iteration: 954000  --  Loss: 1.6212435466950315\n",
      "Iteration: 955000  --  Loss: 1.6212386611318226\n",
      "Iteration: 956000  --  Loss: 1.621234765646134\n",
      "Iteration: 957000  --  Loss: 1.6212306960125187\n",
      "Iteration: 958000  --  Loss: 1.6212219551815594\n",
      "Iteration: 959000  --  Loss: 1.6212169545469823\n",
      "Iteration: 960000  --  Loss: 1.621213604217689\n",
      "Iteration: 961000  --  Loss: 1.621210634147478\n",
      "Iteration: 962000  --  Loss: 1.6212078934350789\n",
      "Iteration: 963000  --  Loss: 1.6212053218858777\n",
      "Iteration: 964000  --  Loss: 1.621202898510501\n",
      "Iteration: 965000  --  Loss: 1.6212005994489522\n",
      "Iteration: 966000  --  Loss: 1.6211984019272445\n",
      "Iteration: 967000  --  Loss: 1.6211962848790975\n",
      "Iteration: 968000  --  Loss: 1.6211942261783998\n",
      "Iteration: 969000  --  Loss: 1.621192201425277\n",
      "Iteration: 970000  --  Loss: 1.6211901562237154\n",
      "Iteration: 971000  --  Loss: 1.6211879412411423\n",
      "Iteration: 972000  --  Loss: 1.621184475380831\n",
      "Iteration: 973000  --  Loss: 1.621161680991188\n",
      "Iteration: 974000  --  Loss: 1.6211518867203658\n",
      "Iteration: 975000  --  Loss: 1.621147441056084\n",
      "Iteration: 976000  --  Loss: 1.6211440178349432\n",
      "Iteration: 977000  --  Loss: 1.6211395298099456\n",
      "Iteration: 978000  --  Loss: 1.6211294485381373\n",
      "Iteration: 979000  --  Loss: 1.621125407068788\n",
      "Iteration: 980000  --  Loss: 1.6211225648234664\n",
      "Iteration: 981000  --  Loss: 1.6211200993639427\n",
      "Iteration: 982000  --  Loss: 1.6211177619755457\n",
      "Iteration: 983000  --  Loss: 1.6211153799480535\n",
      "Iteration: 984000  --  Loss: 1.6211125826574748\n",
      "Iteration: 985000  --  Loss: 1.6211077466053745\n",
      "Iteration: 986000  --  Loss: 1.621097698504871\n",
      "Iteration: 987000  --  Loss: 1.6210906182053828\n",
      "Iteration: 988000  --  Loss: 1.6210866062785754\n",
      "Iteration: 989000  --  Loss: 1.6210834365070539\n",
      "Iteration: 990000  --  Loss: 1.621080624709841\n",
      "Iteration: 991000  --  Loss: 1.6210780202003907\n",
      "Iteration: 992000  --  Loss: 1.6210755256323477\n",
      "Iteration: 993000  --  Loss: 1.6210729274517623\n",
      "Iteration: 994000  --  Loss: 1.621068506784914\n",
      "Iteration: 995000  --  Loss: 1.6210349285087726\n",
      "Iteration: 996000  --  Loss: 1.6210276388814218\n",
      "Iteration: 997000  --  Loss: 1.6210241642194168\n"
     ]
    }
   ],
   "source": [
    "# mnist data set\n",
    "train_img, test_img, train_lbl, test_lbl = load(file_name=\"mnist.data\")\n",
    "    \n",
    "X_train = train_img[:1000] / 255.\n",
    "y_train = train_lbl[:1000].astype(int)\n",
    "X_test = test_img[:100] / 255.\n",
    "y_test = test_lbl[:100].astype(int)\n",
    "\n",
    "n_in = X_train.shape[1]\n",
    "\n",
    "nn = Neural_Network(n_in=n_in, n_out=10)\n",
    "\n",
    "#Randomly assigning activation functions to the hidden layers\n",
    "#List = [\"relu\", \"sigmoid\"]\n",
    "\n",
    "#activation_func = random.choice(List)\n",
    "nn.layer(activation_function = \"relu\", n_neurons=80)\n",
    "nn.layer(activation_function= \"sigmoid\", n_neurons=30)\n",
    "nn.layer(activation_function=\"softmax\", n_neurons=10)\n",
    "\n",
    "nn.train(X_train, y_train, n_epoch=1000000)\n",
    "\n",
    "train_accuracy = nn.Accuracy(X_train,y_train)\n",
    "test_accuracy = nn.Accuracy(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy: {0:0.2f} %'.format(train_accuracy))\n",
    "print('Test Accuracy: {0:0.2f} %'.format(test_accuracy))\n",
    "nn.loss_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
